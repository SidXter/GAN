{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Igq4uFyBYHO"
   },
   "source": [
    "**Things im noticing that students keep getting errors from**\n",
    "\n",
    "- Make sure your indentation matches the videos! Otherwise Python wont understand the code properly and it wont print out the summary of the model like in the video!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJp-D51g0IDd"
   },
   "source": [
    "## **1) Importing Python Packages for GAN**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1k5mFBuzzl2a",
    "outputId": "65c9845b-8889-4905-b174-984a224c94db"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file generated_images already exists.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Dense, Reshape, Flatten\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import numpy as np\n",
    "!mkdir generated_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yr-eZOzg0X79"
   },
   "source": [
    "## **2) Variables for Neural Networks & Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RThZMDruz9cB",
    "outputId": "206a143b-aa26-468b-cd0d-e5e3e1897198"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sidss\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "img_width = 28\n",
    "img_height = 28\n",
    "channels = 1\n",
    "img_shape = (img_width, img_height, channels)\n",
    "latent_dim = 100\n",
    "adam = Adam(lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3bcJZZg0cqy"
   },
   "source": [
    "## **3) Building Generator**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NdiqZpri0iQh",
    "outputId": "654b5c16-b896-4703-d42e-de17e50a4199"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               25856     \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 256)               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 256)              1024      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 256)               0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 256)               0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 784)               201488    \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 362,000\n",
      "Trainable params: 360,464\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_generator():\n",
    "  model = Sequential()\n",
    "\n",
    "  model.add(Dense(256, input_dim=latent_dim))\n",
    "  model.add(LeakyReLU(alpha=0.2))\n",
    "  model.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "  model.add(Dense(256))\n",
    "  model.add(LeakyReLU(alpha=0.2))\n",
    "  model.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "  model.add(Dense(256))\n",
    "  model.add(LeakyReLU(alpha=0.2))\n",
    "  model.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "  model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
    "  model.add(Reshape(img_shape))\n",
    "  \n",
    "  model.summary()\n",
    "  return model\n",
    "\n",
    "generator = build_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bt6QsJCW0mcI"
   },
   "source": [
    "## **4) Building Discriminator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V2JzEAPv0lKt",
    "outputId": "c5a3da1a-67c4-4c53-a4bd-adf096352060"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 512)               401920    \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 512)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_discriminator():\n",
    "  model = Sequential()\n",
    "\n",
    "  model.add(Flatten(input_shape=img_shape))\n",
    "  model.add(Dense(512))\n",
    "  model.add(LeakyReLU(alpha=0.2))\n",
    "  model.add(Dense(256))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "  model.summary()\n",
    "  return model\n",
    "\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TbcKcKmA0q2S"
   },
   "source": [
    "## **5) Connecting Neural Networks to build GAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q0Ue3TEd0xLy",
    "outputId": "7329fc81-deb1-4163-c772-101ba6b158a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential (Sequential)     (None, 28, 28, 1)         362000    \n",
      "                                                                 \n",
      " sequential_1 (Sequential)   (None, 1)                 533505    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 895,505\n",
      "Trainable params: 360,464\n",
      "Non-trainable params: 535,041\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "GAN = Sequential()\n",
    "discriminator.trainable = False\n",
    "GAN.add(generator)\n",
    "GAN.add(discriminator)\n",
    "\n",
    "GAN.compile(loss='binary_crossentropy', optimizer=adam)\n",
    "GAN.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2WaNhBDwRwTG"
   },
   "source": [
    "## **6) Outputting Images**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HQEJ0WbjRppy"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "## **7) Outputting Images**\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import imageio\n",
    "import PIL\n",
    "\n",
    "save_name = 0.00000000\n",
    "\n",
    "def save_imgs(epoch):\n",
    "    r, c = 5, 5\n",
    "    noise = np.random.normal(0, 1, (r * c, latent_dim))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "    global save_name\n",
    "    save_name += 0.00000001\n",
    "    print(\"%.8f\" % save_name)\n",
    "\n",
    "    # Rescale images 0 - 1\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "            # axs[i,j].imshow(gen_imgs[cnt])\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"generated_images/%.8f.png\" % save_name)\n",
    "    print('saved')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tE57Lk5V0xs2"
   },
   "source": [
    "## **7) Training GAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "egSJJvik00Iq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* 0 [D loss: 0.599776, acc: 57.81%] [G loss: 0.665363]\n",
      "0.00000001\n",
      "saved\n",
      "******* 1 [D loss: 0.381291, acc: 69.53%] [G loss: 0.709987]\n",
      "******* 2 [D loss: 0.357298, acc: 77.34%] [G loss: 0.717760]\n",
      "******* 3 [D loss: 0.347358, acc: 78.91%] [G loss: 0.743984]\n",
      "******* 4 [D loss: 0.332754, acc: 78.91%] [G loss: 0.819027]\n",
      "******* 5 [D loss: 0.314205, acc: 83.59%] [G loss: 0.857935]\n",
      "******* 6 [D loss: 0.295380, acc: 87.50%] [G loss: 0.972239]\n",
      "******* 7 [D loss: 0.263352, acc: 94.53%] [G loss: 1.104205]\n",
      "******* 8 [D loss: 0.226399, acc: 97.66%] [G loss: 1.159414]\n",
      "******* 9 [D loss: 0.183166, acc: 96.88%] [G loss: 1.313916]\n",
      "******* 10 [D loss: 0.177527, acc: 98.44%] [G loss: 1.435362]\n",
      "******* 11 [D loss: 0.162689, acc: 97.66%] [G loss: 1.614563]\n",
      "******* 12 [D loss: 0.145017, acc: 99.22%] [G loss: 1.717040]\n",
      "******* 13 [D loss: 0.094976, acc: 100.00%] [G loss: 1.869502]\n",
      "******* 14 [D loss: 0.093277, acc: 100.00%] [G loss: 2.081231]\n",
      "******* 15 [D loss: 0.083475, acc: 100.00%] [G loss: 2.192545]\n",
      "******* 16 [D loss: 0.063401, acc: 100.00%] [G loss: 2.388557]\n",
      "******* 17 [D loss: 0.062520, acc: 100.00%] [G loss: 2.561406]\n",
      "******* 18 [D loss: 0.052785, acc: 100.00%] [G loss: 2.691329]\n",
      "******* 19 [D loss: 0.045473, acc: 100.00%] [G loss: 2.694048]\n",
      "******* 20 [D loss: 0.037780, acc: 100.00%] [G loss: 2.853434]\n",
      "******* 21 [D loss: 0.039887, acc: 100.00%] [G loss: 2.925484]\n",
      "******* 22 [D loss: 0.034588, acc: 100.00%] [G loss: 3.018108]\n",
      "******* 23 [D loss: 0.039344, acc: 100.00%] [G loss: 3.109224]\n",
      "******* 24 [D loss: 0.035052, acc: 100.00%] [G loss: 3.135213]\n",
      "******* 25 [D loss: 0.027984, acc: 100.00%] [G loss: 3.266715]\n",
      "******* 26 [D loss: 0.025113, acc: 100.00%] [G loss: 3.381557]\n",
      "******* 27 [D loss: 0.023101, acc: 100.00%] [G loss: 3.374286]\n",
      "******* 28 [D loss: 0.022550, acc: 100.00%] [G loss: 3.441768]\n",
      "******* 29 [D loss: 0.021545, acc: 100.00%] [G loss: 3.403147]\n",
      "******* 30 [D loss: 0.021369, acc: 100.00%] [G loss: 3.537610]\n",
      "******* 31 [D loss: 0.017345, acc: 100.00%] [G loss: 3.542454]\n",
      "******* 32 [D loss: 0.021031, acc: 100.00%] [G loss: 3.552458]\n",
      "******* 33 [D loss: 0.021051, acc: 100.00%] [G loss: 3.604318]\n",
      "******* 34 [D loss: 0.015592, acc: 100.00%] [G loss: 3.604794]\n",
      "******* 35 [D loss: 0.016961, acc: 100.00%] [G loss: 3.574728]\n",
      "******* 36 [D loss: 0.016676, acc: 100.00%] [G loss: 3.574202]\n",
      "******* 37 [D loss: 0.016522, acc: 100.00%] [G loss: 3.649378]\n",
      "******* 38 [D loss: 0.015086, acc: 100.00%] [G loss: 3.679911]\n",
      "******* 39 [D loss: 0.015999, acc: 100.00%] [G loss: 3.723629]\n",
      "******* 40 [D loss: 0.017628, acc: 100.00%] [G loss: 3.666981]\n",
      "******* 41 [D loss: 0.014827, acc: 100.00%] [G loss: 3.696534]\n",
      "******* 42 [D loss: 0.016945, acc: 100.00%] [G loss: 3.695223]\n",
      "******* 43 [D loss: 0.014817, acc: 100.00%] [G loss: 3.678231]\n",
      "******* 44 [D loss: 0.017024, acc: 100.00%] [G loss: 3.665013]\n",
      "******* 45 [D loss: 0.014138, acc: 100.00%] [G loss: 3.728277]\n",
      "******* 46 [D loss: 0.015246, acc: 100.00%] [G loss: 3.754979]\n",
      "******* 47 [D loss: 0.016371, acc: 100.00%] [G loss: 3.706910]\n",
      "******* 48 [D loss: 0.014798, acc: 100.00%] [G loss: 3.771851]\n",
      "******* 49 [D loss: 0.014842, acc: 100.00%] [G loss: 3.694385]\n",
      "******* 50 [D loss: 0.016395, acc: 100.00%] [G loss: 3.681658]\n",
      "******* 51 [D loss: 0.012326, acc: 100.00%] [G loss: 3.705107]\n",
      "******* 52 [D loss: 0.015785, acc: 100.00%] [G loss: 3.735320]\n",
      "******* 53 [D loss: 0.014786, acc: 100.00%] [G loss: 3.726836]\n",
      "******* 54 [D loss: 0.015464, acc: 100.00%] [G loss: 3.784724]\n",
      "******* 55 [D loss: 0.017066, acc: 100.00%] [G loss: 3.821378]\n",
      "******* 56 [D loss: 0.012815, acc: 100.00%] [G loss: 3.814779]\n",
      "******* 57 [D loss: 0.012643, acc: 100.00%] [G loss: 3.823309]\n",
      "******* 58 [D loss: 0.013214, acc: 100.00%] [G loss: 3.846837]\n",
      "******* 59 [D loss: 0.013020, acc: 100.00%] [G loss: 3.805042]\n",
      "******* 60 [D loss: 0.012387, acc: 100.00%] [G loss: 3.843722]\n",
      "******* 61 [D loss: 0.014136, acc: 100.00%] [G loss: 3.837368]\n",
      "******* 62 [D loss: 0.013280, acc: 100.00%] [G loss: 3.785738]\n",
      "******* 63 [D loss: 0.015083, acc: 100.00%] [G loss: 3.826595]\n",
      "******* 64 [D loss: 0.013407, acc: 100.00%] [G loss: 3.842932]\n",
      "******* 65 [D loss: 0.013478, acc: 100.00%] [G loss: 3.866335]\n",
      "******* 66 [D loss: 0.013145, acc: 100.00%] [G loss: 3.887935]\n",
      "******* 67 [D loss: 0.011686, acc: 100.00%] [G loss: 3.863246]\n",
      "******* 68 [D loss: 0.013427, acc: 100.00%] [G loss: 3.861513]\n",
      "******* 69 [D loss: 0.011576, acc: 100.00%] [G loss: 3.859131]\n",
      "******* 70 [D loss: 0.013675, acc: 100.00%] [G loss: 3.905617]\n",
      "******* 71 [D loss: 0.013238, acc: 100.00%] [G loss: 3.944132]\n",
      "******* 72 [D loss: 0.010826, acc: 100.00%] [G loss: 3.871836]\n",
      "******* 73 [D loss: 0.013756, acc: 100.00%] [G loss: 3.944154]\n",
      "******* 74 [D loss: 0.010818, acc: 100.00%] [G loss: 3.993997]\n",
      "******* 75 [D loss: 0.010252, acc: 100.00%] [G loss: 3.960810]\n",
      "******* 76 [D loss: 0.010400, acc: 100.00%] [G loss: 4.005838]\n",
      "******* 77 [D loss: 0.011657, acc: 100.00%] [G loss: 3.904184]\n",
      "******* 78 [D loss: 0.010731, acc: 100.00%] [G loss: 3.955447]\n",
      "******* 79 [D loss: 0.011856, acc: 100.00%] [G loss: 4.038921]\n",
      "******* 80 [D loss: 0.011400, acc: 100.00%] [G loss: 3.967508]\n",
      "******* 81 [D loss: 0.010763, acc: 100.00%] [G loss: 4.003947]\n",
      "******* 82 [D loss: 0.012056, acc: 100.00%] [G loss: 3.949569]\n",
      "******* 83 [D loss: 0.011749, acc: 100.00%] [G loss: 4.001003]\n",
      "******* 84 [D loss: 0.011122, acc: 100.00%] [G loss: 4.005525]\n",
      "******* 85 [D loss: 0.010211, acc: 100.00%] [G loss: 4.019207]\n",
      "******* 86 [D loss: 0.010798, acc: 100.00%] [G loss: 3.981828]\n",
      "******* 87 [D loss: 0.011305, acc: 100.00%] [G loss: 4.017797]\n",
      "******* 88 [D loss: 0.011834, acc: 100.00%] [G loss: 4.059384]\n",
      "******* 89 [D loss: 0.010865, acc: 100.00%] [G loss: 4.009392]\n",
      "******* 90 [D loss: 0.011102, acc: 100.00%] [G loss: 4.090766]\n",
      "******* 91 [D loss: 0.010884, acc: 100.00%] [G loss: 4.098495]\n",
      "******* 92 [D loss: 0.008823, acc: 100.00%] [G loss: 4.022742]\n",
      "******* 93 [D loss: 0.012616, acc: 100.00%] [G loss: 4.033079]\n",
      "******* 94 [D loss: 0.010760, acc: 100.00%] [G loss: 4.107142]\n",
      "******* 95 [D loss: 0.012301, acc: 100.00%] [G loss: 4.120715]\n",
      "******* 96 [D loss: 0.010206, acc: 100.00%] [G loss: 4.100574]\n",
      "******* 97 [D loss: 0.010453, acc: 100.00%] [G loss: 4.072190]\n",
      "******* 98 [D loss: 0.010403, acc: 100.00%] [G loss: 4.090622]\n",
      "******* 99 [D loss: 0.011714, acc: 100.00%] [G loss: 4.105201]\n",
      "******* 100 [D loss: 0.011618, acc: 100.00%] [G loss: 4.087675]\n",
      "******* 101 [D loss: 0.010508, acc: 100.00%] [G loss: 4.146115]\n",
      "******* 102 [D loss: 0.009818, acc: 100.00%] [G loss: 4.163376]\n",
      "******* 103 [D loss: 0.009762, acc: 100.00%] [G loss: 4.127979]\n",
      "******* 104 [D loss: 0.011307, acc: 100.00%] [G loss: 4.173371]\n",
      "******* 105 [D loss: 0.010126, acc: 100.00%] [G loss: 4.105489]\n",
      "******* 106 [D loss: 0.009712, acc: 100.00%] [G loss: 4.342364]\n",
      "******* 107 [D loss: 0.009182, acc: 100.00%] [G loss: 4.204072]\n",
      "******* 108 [D loss: 0.008521, acc: 100.00%] [G loss: 4.208252]\n",
      "******* 109 [D loss: 0.009505, acc: 100.00%] [G loss: 4.194371]\n",
      "******* 110 [D loss: 0.008606, acc: 100.00%] [G loss: 4.186472]\n",
      "******* 111 [D loss: 0.009663, acc: 100.00%] [G loss: 4.165969]\n",
      "******* 112 [D loss: 0.009231, acc: 100.00%] [G loss: 4.231591]\n",
      "******* 113 [D loss: 0.008842, acc: 100.00%] [G loss: 4.221320]\n",
      "******* 114 [D loss: 0.009325, acc: 100.00%] [G loss: 4.291611]\n",
      "******* 115 [D loss: 0.009910, acc: 100.00%] [G loss: 4.258045]\n",
      "******* 116 [D loss: 0.009797, acc: 100.00%] [G loss: 4.323793]\n",
      "******* 117 [D loss: 0.009484, acc: 100.00%] [G loss: 4.300914]\n",
      "******* 118 [D loss: 0.008949, acc: 100.00%] [G loss: 4.330800]\n",
      "******* 119 [D loss: 0.009653, acc: 100.00%] [G loss: 4.281453]\n",
      "******* 120 [D loss: 0.008697, acc: 100.00%] [G loss: 4.228703]\n",
      "******* 121 [D loss: 0.008517, acc: 100.00%] [G loss: 4.221887]\n",
      "******* 122 [D loss: 0.010586, acc: 100.00%] [G loss: 4.289472]\n",
      "******* 123 [D loss: 0.008850, acc: 100.00%] [G loss: 4.298409]\n",
      "******* 124 [D loss: 0.009787, acc: 100.00%] [G loss: 4.354018]\n",
      "******* 125 [D loss: 0.009833, acc: 100.00%] [G loss: 4.304877]\n",
      "******* 126 [D loss: 0.010202, acc: 100.00%] [G loss: 4.351572]\n",
      "******* 127 [D loss: 0.008099, acc: 100.00%] [G loss: 4.305276]\n",
      "******* 128 [D loss: 0.009358, acc: 100.00%] [G loss: 4.448826]\n",
      "******* 129 [D loss: 0.006564, acc: 100.00%] [G loss: 4.381513]\n",
      "******* 130 [D loss: 0.010741, acc: 100.00%] [G loss: 4.309239]\n",
      "******* 131 [D loss: 0.009744, acc: 100.00%] [G loss: 4.356180]\n",
      "******* 132 [D loss: 0.008759, acc: 100.00%] [G loss: 4.315541]\n",
      "******* 133 [D loss: 0.008615, acc: 100.00%] [G loss: 4.414542]\n",
      "******* 134 [D loss: 0.007548, acc: 100.00%] [G loss: 4.332534]\n",
      "******* 135 [D loss: 0.007195, acc: 100.00%] [G loss: 4.422852]\n",
      "******* 136 [D loss: 0.007018, acc: 100.00%] [G loss: 4.484612]\n",
      "******* 137 [D loss: 0.007299, acc: 100.00%] [G loss: 4.461921]\n",
      "******* 138 [D loss: 0.008385, acc: 100.00%] [G loss: 4.403862]\n",
      "******* 139 [D loss: 0.009649, acc: 100.00%] [G loss: 4.471891]\n",
      "******* 140 [D loss: 0.008057, acc: 100.00%] [G loss: 4.492623]\n",
      "******* 141 [D loss: 0.007916, acc: 100.00%] [G loss: 4.470672]\n",
      "******* 142 [D loss: 0.008016, acc: 100.00%] [G loss: 4.414741]\n",
      "******* 143 [D loss: 0.006865, acc: 100.00%] [G loss: 4.484722]\n",
      "******* 144 [D loss: 0.007444, acc: 100.00%] [G loss: 4.378087]\n",
      "******* 145 [D loss: 0.009174, acc: 100.00%] [G loss: 4.386025]\n",
      "******* 146 [D loss: 0.006792, acc: 100.00%] [G loss: 4.438870]\n",
      "******* 147 [D loss: 0.008080, acc: 100.00%] [G loss: 4.448666]\n",
      "******* 148 [D loss: 0.007319, acc: 100.00%] [G loss: 4.470168]\n",
      "******* 149 [D loss: 0.009371, acc: 100.00%] [G loss: 4.431536]\n",
      "******* 150 [D loss: 0.008089, acc: 100.00%] [G loss: 4.509941]\n",
      "******* 151 [D loss: 0.006608, acc: 100.00%] [G loss: 4.501536]\n",
      "******* 152 [D loss: 0.008856, acc: 100.00%] [G loss: 4.484764]\n",
      "******* 153 [D loss: 0.007477, acc: 100.00%] [G loss: 4.413397]\n",
      "******* 154 [D loss: 0.007211, acc: 100.00%] [G loss: 4.392483]\n",
      "******* 155 [D loss: 0.008473, acc: 100.00%] [G loss: 4.473616]\n",
      "******* 156 [D loss: 0.008873, acc: 100.00%] [G loss: 4.433886]\n",
      "******* 157 [D loss: 0.007020, acc: 100.00%] [G loss: 4.473001]\n",
      "******* 158 [D loss: 0.008221, acc: 100.00%] [G loss: 4.451104]\n",
      "******* 159 [D loss: 0.008442, acc: 100.00%] [G loss: 4.451595]\n",
      "******* 160 [D loss: 0.006987, acc: 100.00%] [G loss: 4.495054]\n",
      "******* 161 [D loss: 0.007463, acc: 100.00%] [G loss: 4.522767]\n",
      "******* 162 [D loss: 0.008845, acc: 100.00%] [G loss: 4.464997]\n",
      "******* 163 [D loss: 0.008184, acc: 100.00%] [G loss: 4.431169]\n",
      "******* 164 [D loss: 0.008305, acc: 100.00%] [G loss: 4.471899]\n",
      "******* 165 [D loss: 0.007890, acc: 100.00%] [G loss: 4.472620]\n",
      "******* 166 [D loss: 0.006967, acc: 100.00%] [G loss: 4.505194]\n",
      "******* 167 [D loss: 0.008613, acc: 100.00%] [G loss: 4.514475]\n",
      "******* 168 [D loss: 0.007372, acc: 100.00%] [G loss: 4.536296]\n",
      "******* 169 [D loss: 0.008001, acc: 100.00%] [G loss: 4.485549]\n",
      "******* 170 [D loss: 0.007551, acc: 100.00%] [G loss: 4.473914]\n",
      "******* 171 [D loss: 0.010557, acc: 100.00%] [G loss: 4.454541]\n",
      "******* 172 [D loss: 0.007590, acc: 100.00%] [G loss: 4.478437]\n",
      "******* 173 [D loss: 0.006708, acc: 100.00%] [G loss: 4.557675]\n",
      "******* 174 [D loss: 0.010775, acc: 100.00%] [G loss: 4.507819]\n",
      "******* 175 [D loss: 0.008886, acc: 100.00%] [G loss: 4.566803]\n",
      "******* 176 [D loss: 0.006384, acc: 100.00%] [G loss: 4.472765]\n",
      "******* 177 [D loss: 0.010528, acc: 100.00%] [G loss: 4.578023]\n",
      "******* 178 [D loss: 0.006555, acc: 100.00%] [G loss: 4.545115]\n",
      "******* 179 [D loss: 0.007998, acc: 100.00%] [G loss: 4.548079]\n",
      "******* 180 [D loss: 0.006949, acc: 100.00%] [G loss: 4.521846]\n",
      "******* 181 [D loss: 0.008031, acc: 100.00%] [G loss: 4.605843]\n",
      "******* 182 [D loss: 0.007048, acc: 100.00%] [G loss: 4.523057]\n",
      "******* 183 [D loss: 0.008360, acc: 100.00%] [G loss: 4.495089]\n",
      "******* 184 [D loss: 0.008337, acc: 100.00%] [G loss: 4.459400]\n",
      "******* 185 [D loss: 0.008810, acc: 100.00%] [G loss: 4.461456]\n",
      "******* 186 [D loss: 0.009770, acc: 100.00%] [G loss: 4.586929]\n",
      "******* 187 [D loss: 0.009722, acc: 100.00%] [G loss: 4.517535]\n",
      "******* 188 [D loss: 0.009291, acc: 100.00%] [G loss: 4.457223]\n",
      "******* 189 [D loss: 0.009706, acc: 100.00%] [G loss: 4.556100]\n",
      "******* 190 [D loss: 0.010463, acc: 100.00%] [G loss: 4.545294]\n",
      "******* 191 [D loss: 0.010029, acc: 100.00%] [G loss: 4.503160]\n",
      "******* 192 [D loss: 0.007268, acc: 100.00%] [G loss: 4.658185]\n",
      "******* 193 [D loss: 0.006252, acc: 100.00%] [G loss: 4.592009]\n",
      "******* 194 [D loss: 0.009263, acc: 100.00%] [G loss: 4.599246]\n",
      "******* 195 [D loss: 0.007619, acc: 100.00%] [G loss: 4.634138]\n",
      "******* 196 [D loss: 0.009562, acc: 100.00%] [G loss: 4.542220]\n",
      "******* 197 [D loss: 0.010023, acc: 100.00%] [G loss: 4.574389]\n",
      "******* 198 [D loss: 0.008897, acc: 100.00%] [G loss: 4.593435]\n",
      "******* 199 [D loss: 0.009899, acc: 100.00%] [G loss: 4.540532]\n",
      "******* 200 [D loss: 0.009095, acc: 100.00%] [G loss: 4.485259]\n",
      "0.00000002\n",
      "saved\n",
      "******* 201 [D loss: 0.008394, acc: 100.00%] [G loss: 4.520741]\n",
      "******* 202 [D loss: 0.008724, acc: 100.00%] [G loss: 4.590428]\n",
      "******* 203 [D loss: 0.011802, acc: 100.00%] [G loss: 4.526533]\n",
      "******* 204 [D loss: 0.007633, acc: 100.00%] [G loss: 4.583422]\n",
      "******* 205 [D loss: 0.007463, acc: 100.00%] [G loss: 4.549242]\n",
      "******* 206 [D loss: 0.010011, acc: 100.00%] [G loss: 4.500141]\n",
      "******* 207 [D loss: 0.010637, acc: 100.00%] [G loss: 4.404427]\n",
      "******* 208 [D loss: 0.009351, acc: 100.00%] [G loss: 4.540286]\n",
      "******* 209 [D loss: 0.009159, acc: 100.00%] [G loss: 4.615765]\n",
      "******* 210 [D loss: 0.010074, acc: 100.00%] [G loss: 4.618602]\n",
      "******* 211 [D loss: 0.010251, acc: 100.00%] [G loss: 4.617556]\n",
      "******* 212 [D loss: 0.009329, acc: 100.00%] [G loss: 4.703131]\n",
      "******* 213 [D loss: 0.009387, acc: 100.00%] [G loss: 4.574248]\n",
      "******* 214 [D loss: 0.008139, acc: 100.00%] [G loss: 4.528968]\n",
      "******* 215 [D loss: 0.007501, acc: 100.00%] [G loss: 4.573910]\n",
      "******* 216 [D loss: 0.009503, acc: 100.00%] [G loss: 4.448249]\n",
      "******* 217 [D loss: 0.011809, acc: 100.00%] [G loss: 4.495181]\n",
      "******* 218 [D loss: 0.013007, acc: 100.00%] [G loss: 4.435255]\n",
      "******* 219 [D loss: 0.011188, acc: 100.00%] [G loss: 4.560985]\n",
      "******* 220 [D loss: 0.013881, acc: 100.00%] [G loss: 4.499393]\n",
      "******* 221 [D loss: 0.010397, acc: 100.00%] [G loss: 4.461265]\n",
      "******* 222 [D loss: 0.010638, acc: 100.00%] [G loss: 4.503494]\n",
      "******* 223 [D loss: 0.012344, acc: 100.00%] [G loss: 4.445923]\n",
      "******* 224 [D loss: 0.009805, acc: 100.00%] [G loss: 4.472999]\n",
      "******* 225 [D loss: 0.010041, acc: 100.00%] [G loss: 4.465887]\n",
      "******* 226 [D loss: 0.014067, acc: 100.00%] [G loss: 4.508624]\n",
      "******* 227 [D loss: 0.012210, acc: 100.00%] [G loss: 4.598285]\n",
      "******* 228 [D loss: 0.013111, acc: 100.00%] [G loss: 4.642212]\n",
      "******* 229 [D loss: 0.010738, acc: 100.00%] [G loss: 4.639712]\n",
      "******* 230 [D loss: 0.010677, acc: 100.00%] [G loss: 4.532827]\n",
      "******* 231 [D loss: 0.013224, acc: 100.00%] [G loss: 4.569677]\n",
      "******* 232 [D loss: 0.010814, acc: 100.00%] [G loss: 4.556031]\n",
      "******* 233 [D loss: 0.013194, acc: 100.00%] [G loss: 4.574852]\n",
      "******* 234 [D loss: 0.007904, acc: 100.00%] [G loss: 4.551537]\n",
      "******* 235 [D loss: 0.011900, acc: 100.00%] [G loss: 4.449481]\n",
      "******* 236 [D loss: 0.012663, acc: 100.00%] [G loss: 4.542527]\n",
      "******* 237 [D loss: 0.009368, acc: 100.00%] [G loss: 4.597653]\n",
      "******* 238 [D loss: 0.013336, acc: 100.00%] [G loss: 4.478300]\n",
      "******* 239 [D loss: 0.010420, acc: 100.00%] [G loss: 4.438411]\n",
      "******* 240 [D loss: 0.011264, acc: 100.00%] [G loss: 4.431260]\n",
      "******* 241 [D loss: 0.015430, acc: 100.00%] [G loss: 4.370426]\n",
      "******* 242 [D loss: 0.011580, acc: 100.00%] [G loss: 4.443890]\n",
      "******* 243 [D loss: 0.012723, acc: 100.00%] [G loss: 4.408464]\n",
      "******* 244 [D loss: 0.017631, acc: 100.00%] [G loss: 4.395182]\n",
      "******* 245 [D loss: 0.015401, acc: 100.00%] [G loss: 4.602149]\n",
      "******* 246 [D loss: 0.017648, acc: 100.00%] [G loss: 4.679785]\n",
      "******* 247 [D loss: 0.017795, acc: 100.00%] [G loss: 4.611159]\n",
      "******* 248 [D loss: 0.011637, acc: 100.00%] [G loss: 4.563423]\n",
      "******* 249 [D loss: 0.016626, acc: 100.00%] [G loss: 4.620109]\n",
      "******* 250 [D loss: 0.021448, acc: 100.00%] [G loss: 4.576207]\n",
      "******* 251 [D loss: 0.019892, acc: 100.00%] [G loss: 4.320250]\n",
      "******* 252 [D loss: 0.020539, acc: 100.00%] [G loss: 4.374353]\n",
      "******* 253 [D loss: 0.012616, acc: 100.00%] [G loss: 4.434173]\n",
      "******* 254 [D loss: 0.016999, acc: 100.00%] [G loss: 4.479791]\n",
      "******* 255 [D loss: 0.018365, acc: 100.00%] [G loss: 4.413860]\n",
      "******* 256 [D loss: 0.015461, acc: 100.00%] [G loss: 4.464772]\n",
      "******* 257 [D loss: 0.022224, acc: 100.00%] [G loss: 4.548121]\n",
      "******* 258 [D loss: 0.016460, acc: 100.00%] [G loss: 4.440784]\n",
      "******* 259 [D loss: 0.017497, acc: 100.00%] [G loss: 4.468024]\n",
      "******* 260 [D loss: 0.013847, acc: 100.00%] [G loss: 4.317959]\n",
      "******* 261 [D loss: 0.018174, acc: 100.00%] [G loss: 4.321278]\n",
      "******* 262 [D loss: 0.020833, acc: 100.00%] [G loss: 4.393307]\n",
      "******* 263 [D loss: 0.017646, acc: 100.00%] [G loss: 4.383391]\n",
      "******* 264 [D loss: 0.028067, acc: 100.00%] [G loss: 4.459132]\n",
      "******* 265 [D loss: 0.032792, acc: 100.00%] [G loss: 4.335497]\n",
      "******* 266 [D loss: 0.016760, acc: 100.00%] [G loss: 4.361393]\n",
      "******* 267 [D loss: 0.024615, acc: 100.00%] [G loss: 4.446288]\n",
      "******* 268 [D loss: 0.019990, acc: 100.00%] [G loss: 4.350976]\n",
      "******* 269 [D loss: 0.019764, acc: 100.00%] [G loss: 4.365591]\n",
      "******* 270 [D loss: 0.024667, acc: 100.00%] [G loss: 4.362665]\n",
      "******* 271 [D loss: 0.033854, acc: 99.22%] [G loss: 4.368475]\n",
      "******* 272 [D loss: 0.024349, acc: 100.00%] [G loss: 4.332660]\n",
      "******* 273 [D loss: 0.016850, acc: 100.00%] [G loss: 4.406138]\n",
      "******* 274 [D loss: 0.023878, acc: 100.00%] [G loss: 4.402442]\n",
      "******* 275 [D loss: 0.019926, acc: 100.00%] [G loss: 4.327338]\n",
      "******* 276 [D loss: 0.024188, acc: 100.00%] [G loss: 4.285446]\n",
      "******* 277 [D loss: 0.022070, acc: 100.00%] [G loss: 4.440105]\n",
      "******* 278 [D loss: 0.024305, acc: 100.00%] [G loss: 4.173677]\n",
      "******* 279 [D loss: 0.037519, acc: 99.22%] [G loss: 4.196184]\n",
      "******* 280 [D loss: 0.023421, acc: 100.00%] [G loss: 4.300384]\n",
      "******* 281 [D loss: 0.040359, acc: 100.00%] [G loss: 4.192750]\n",
      "******* 282 [D loss: 0.017588, acc: 100.00%] [G loss: 4.525123]\n",
      "******* 283 [D loss: 0.031594, acc: 100.00%] [G loss: 4.388150]\n",
      "******* 284 [D loss: 0.024061, acc: 100.00%] [G loss: 4.254559]\n",
      "******* 285 [D loss: 0.036816, acc: 100.00%] [G loss: 4.131468]\n",
      "******* 286 [D loss: 0.038266, acc: 100.00%] [G loss: 4.174661]\n",
      "******* 287 [D loss: 0.034062, acc: 100.00%] [G loss: 4.204223]\n",
      "******* 288 [D loss: 0.029393, acc: 100.00%] [G loss: 4.307211]\n",
      "******* 289 [D loss: 0.032541, acc: 100.00%] [G loss: 4.344286]\n",
      "******* 290 [D loss: 0.040087, acc: 100.00%] [G loss: 4.329844]\n",
      "******* 291 [D loss: 0.040454, acc: 100.00%] [G loss: 4.136981]\n",
      "******* 292 [D loss: 0.038914, acc: 100.00%] [G loss: 4.262067]\n",
      "******* 293 [D loss: 0.034710, acc: 99.22%] [G loss: 4.061090]\n",
      "******* 294 [D loss: 0.030263, acc: 100.00%] [G loss: 4.095851]\n",
      "******* 295 [D loss: 0.030676, acc: 100.00%] [G loss: 4.191473]\n",
      "******* 296 [D loss: 0.029818, acc: 100.00%] [G loss: 4.400816]\n",
      "******* 297 [D loss: 0.024853, acc: 100.00%] [G loss: 4.314233]\n",
      "******* 298 [D loss: 0.041844, acc: 100.00%] [G loss: 4.298877]\n",
      "******* 299 [D loss: 0.024749, acc: 100.00%] [G loss: 4.030882]\n",
      "******* 300 [D loss: 0.037753, acc: 100.00%] [G loss: 3.970345]\n",
      "******* 301 [D loss: 0.033850, acc: 100.00%] [G loss: 3.950943]\n",
      "******* 302 [D loss: 0.054205, acc: 99.22%] [G loss: 3.983472]\n",
      "******* 303 [D loss: 0.032076, acc: 100.00%] [G loss: 3.789984]\n",
      "******* 304 [D loss: 0.036029, acc: 99.22%] [G loss: 3.978422]\n",
      "******* 305 [D loss: 0.027955, acc: 100.00%] [G loss: 4.100644]\n",
      "******* 306 [D loss: 0.030535, acc: 100.00%] [G loss: 4.046906]\n",
      "******* 307 [D loss: 0.035673, acc: 99.22%] [G loss: 4.002292]\n",
      "******* 308 [D loss: 0.048277, acc: 100.00%] [G loss: 3.972577]\n",
      "******* 309 [D loss: 0.060433, acc: 97.66%] [G loss: 3.909338]\n",
      "******* 310 [D loss: 0.028625, acc: 100.00%] [G loss: 4.116457]\n",
      "******* 311 [D loss: 0.056232, acc: 99.22%] [G loss: 4.009868]\n",
      "******* 312 [D loss: 0.027352, acc: 100.00%] [G loss: 4.211005]\n",
      "******* 313 [D loss: 0.033229, acc: 100.00%] [G loss: 4.163996]\n",
      "******* 314 [D loss: 0.039678, acc: 99.22%] [G loss: 4.174247]\n",
      "******* 315 [D loss: 0.037431, acc: 100.00%] [G loss: 3.766305]\n",
      "******* 316 [D loss: 0.023966, acc: 100.00%] [G loss: 4.031431]\n",
      "******* 317 [D loss: 0.046014, acc: 100.00%] [G loss: 3.939342]\n",
      "******* 318 [D loss: 0.044632, acc: 100.00%] [G loss: 3.961223]\n",
      "******* 319 [D loss: 0.037492, acc: 100.00%] [G loss: 3.932992]\n",
      "******* 320 [D loss: 0.037584, acc: 100.00%] [G loss: 3.985781]\n",
      "******* 321 [D loss: 0.054984, acc: 99.22%] [G loss: 3.783080]\n",
      "******* 322 [D loss: 0.035578, acc: 100.00%] [G loss: 3.853423]\n",
      "******* 323 [D loss: 0.032736, acc: 100.00%] [G loss: 3.717290]\n",
      "******* 324 [D loss: 0.045943, acc: 99.22%] [G loss: 4.113843]\n",
      "******* 325 [D loss: 0.040999, acc: 98.44%] [G loss: 4.185949]\n",
      "******* 326 [D loss: 0.043911, acc: 100.00%] [G loss: 3.907125]\n",
      "******* 327 [D loss: 0.039537, acc: 100.00%] [G loss: 3.957350]\n",
      "******* 328 [D loss: 0.046199, acc: 100.00%] [G loss: 3.971336]\n",
      "******* 329 [D loss: 0.037917, acc: 100.00%] [G loss: 3.796407]\n",
      "******* 330 [D loss: 0.036959, acc: 100.00%] [G loss: 4.003872]\n",
      "******* 331 [D loss: 0.035163, acc: 100.00%] [G loss: 3.919666]\n",
      "******* 332 [D loss: 0.045504, acc: 100.00%] [G loss: 3.762001]\n",
      "******* 333 [D loss: 0.036339, acc: 99.22%] [G loss: 3.885796]\n",
      "******* 334 [D loss: 0.034457, acc: 100.00%] [G loss: 3.991260]\n",
      "******* 335 [D loss: 0.041578, acc: 100.00%] [G loss: 3.979066]\n",
      "******* 336 [D loss: 0.048447, acc: 99.22%] [G loss: 4.059966]\n",
      "******* 337 [D loss: 0.052190, acc: 99.22%] [G loss: 3.854582]\n",
      "******* 338 [D loss: 0.043052, acc: 98.44%] [G loss: 3.738201]\n",
      "******* 339 [D loss: 0.073871, acc: 98.44%] [G loss: 3.490718]\n",
      "******* 340 [D loss: 0.049794, acc: 99.22%] [G loss: 4.066405]\n",
      "******* 341 [D loss: 0.056605, acc: 98.44%] [G loss: 3.809409]\n",
      "******* 342 [D loss: 0.047041, acc: 100.00%] [G loss: 3.927090]\n",
      "******* 343 [D loss: 0.057575, acc: 99.22%] [G loss: 3.638003]\n",
      "******* 344 [D loss: 0.039579, acc: 100.00%] [G loss: 3.692976]\n",
      "******* 345 [D loss: 0.052667, acc: 100.00%] [G loss: 3.569439]\n",
      "******* 346 [D loss: 0.044630, acc: 99.22%] [G loss: 3.758687]\n",
      "******* 347 [D loss: 0.070267, acc: 98.44%] [G loss: 3.447822]\n",
      "******* 348 [D loss: 0.068232, acc: 98.44%] [G loss: 3.995749]\n",
      "******* 349 [D loss: 0.051919, acc: 100.00%] [G loss: 4.164701]\n",
      "******* 350 [D loss: 0.040331, acc: 100.00%] [G loss: 4.030840]\n",
      "******* 351 [D loss: 0.126733, acc: 97.66%] [G loss: 3.556893]\n",
      "******* 352 [D loss: 0.063347, acc: 99.22%] [G loss: 3.060996]\n",
      "******* 353 [D loss: 0.091441, acc: 97.66%] [G loss: 3.371063]\n",
      "******* 354 [D loss: 0.055538, acc: 99.22%] [G loss: 4.007891]\n",
      "******* 355 [D loss: 0.064821, acc: 99.22%] [G loss: 4.310515]\n",
      "******* 356 [D loss: 0.080301, acc: 96.09%] [G loss: 3.536294]\n",
      "******* 357 [D loss: 0.084176, acc: 99.22%] [G loss: 3.226178]\n",
      "******* 358 [D loss: 0.078257, acc: 98.44%] [G loss: 3.492007]\n",
      "******* 359 [D loss: 0.074026, acc: 98.44%] [G loss: 3.814425]\n",
      "******* 360 [D loss: 0.037723, acc: 100.00%] [G loss: 4.480987]\n",
      "******* 361 [D loss: 0.054823, acc: 97.66%] [G loss: 4.223195]\n",
      "******* 362 [D loss: 0.064362, acc: 99.22%] [G loss: 3.698957]\n",
      "******* 363 [D loss: 0.050990, acc: 100.00%] [G loss: 3.552130]\n",
      "******* 364 [D loss: 0.088154, acc: 98.44%] [G loss: 3.409971]\n",
      "******* 365 [D loss: 0.044904, acc: 100.00%] [G loss: 3.440154]\n",
      "******* 366 [D loss: 0.052134, acc: 98.44%] [G loss: 3.924886]\n",
      "******* 367 [D loss: 0.050317, acc: 99.22%] [G loss: 4.103476]\n",
      "******* 368 [D loss: 0.055292, acc: 99.22%] [G loss: 4.399898]\n",
      "******* 369 [D loss: 0.063788, acc: 99.22%] [G loss: 3.753272]\n",
      "******* 370 [D loss: 0.065696, acc: 97.66%] [G loss: 3.572112]\n",
      "******* 371 [D loss: 0.063374, acc: 98.44%] [G loss: 3.489933]\n",
      "******* 372 [D loss: 0.058256, acc: 97.66%] [G loss: 3.950293]\n",
      "******* 373 [D loss: 0.038424, acc: 100.00%] [G loss: 4.324925]\n",
      "******* 374 [D loss: 0.040622, acc: 100.00%] [G loss: 4.466414]\n",
      "******* 375 [D loss: 0.093431, acc: 97.66%] [G loss: 3.909149]\n",
      "******* 376 [D loss: 0.062607, acc: 97.66%] [G loss: 3.517483]\n",
      "******* 377 [D loss: 0.086543, acc: 96.88%] [G loss: 3.590256]\n",
      "******* 378 [D loss: 0.041178, acc: 100.00%] [G loss: 4.025655]\n",
      "******* 379 [D loss: 0.032412, acc: 100.00%] [G loss: 4.670304]\n",
      "******* 380 [D loss: 0.030666, acc: 99.22%] [G loss: 4.602573]\n",
      "******* 381 [D loss: 0.067413, acc: 97.66%] [G loss: 4.042027]\n",
      "******* 382 [D loss: 0.048811, acc: 100.00%] [G loss: 3.696474]\n",
      "******* 383 [D loss: 0.086265, acc: 96.88%] [G loss: 3.348130]\n",
      "******* 384 [D loss: 0.079626, acc: 97.66%] [G loss: 3.631759]\n",
      "******* 385 [D loss: 0.056051, acc: 99.22%] [G loss: 4.324000]\n",
      "******* 386 [D loss: 0.055294, acc: 98.44%] [G loss: 4.493973]\n",
      "******* 387 [D loss: 0.030038, acc: 100.00%] [G loss: 4.647173]\n",
      "******* 388 [D loss: 0.028066, acc: 100.00%] [G loss: 4.320886]\n",
      "******* 389 [D loss: 0.038607, acc: 100.00%] [G loss: 3.823369]\n",
      "******* 390 [D loss: 0.030567, acc: 100.00%] [G loss: 4.241594]\n",
      "******* 391 [D loss: 0.031298, acc: 100.00%] [G loss: 4.321936]\n",
      "******* 392 [D loss: 0.039570, acc: 100.00%] [G loss: 3.985382]\n",
      "******* 393 [D loss: 0.032357, acc: 100.00%] [G loss: 4.583921]\n",
      "******* 394 [D loss: 0.039125, acc: 100.00%] [G loss: 4.248688]\n",
      "******* 395 [D loss: 0.044967, acc: 99.22%] [G loss: 3.866914]\n",
      "******* 396 [D loss: 0.059218, acc: 98.44%] [G loss: 3.826990]\n",
      "******* 397 [D loss: 0.047133, acc: 99.22%] [G loss: 4.075553]\n",
      "******* 398 [D loss: 0.052018, acc: 98.44%] [G loss: 4.355436]\n",
      "******* 399 [D loss: 0.029421, acc: 100.00%] [G loss: 4.607340]\n",
      "******* 400 [D loss: 0.064300, acc: 98.44%] [G loss: 3.948876]\n",
      "0.00000003\n",
      "saved\n",
      "******* 401 [D loss: 0.060595, acc: 99.22%] [G loss: 3.872806]\n",
      "******* 402 [D loss: 0.059196, acc: 100.00%] [G loss: 3.706443]\n",
      "******* 403 [D loss: 0.029565, acc: 100.00%] [G loss: 4.032372]\n",
      "******* 404 [D loss: 0.024737, acc: 100.00%] [G loss: 4.788726]\n",
      "******* 405 [D loss: 0.020130, acc: 100.00%] [G loss: 4.807787]\n",
      "******* 406 [D loss: 0.078720, acc: 98.44%] [G loss: 4.397750]\n",
      "******* 407 [D loss: 0.026418, acc: 100.00%] [G loss: 4.443606]\n",
      "******* 408 [D loss: 0.013536, acc: 100.00%] [G loss: 4.672689]\n",
      "******* 409 [D loss: 0.033311, acc: 100.00%] [G loss: 4.272464]\n",
      "******* 410 [D loss: 0.027869, acc: 100.00%] [G loss: 4.652899]\n",
      "******* 411 [D loss: 0.019670, acc: 100.00%] [G loss: 4.978638]\n",
      "******* 412 [D loss: 0.038876, acc: 100.00%] [G loss: 4.953485]\n",
      "******* 413 [D loss: 0.065950, acc: 97.66%] [G loss: 4.134721]\n",
      "******* 414 [D loss: 0.043439, acc: 99.22%] [G loss: 3.777459]\n",
      "******* 415 [D loss: 0.054886, acc: 99.22%] [G loss: 3.715373]\n",
      "******* 416 [D loss: 0.050081, acc: 99.22%] [G loss: 4.472011]\n",
      "******* 417 [D loss: 0.016263, acc: 100.00%] [G loss: 4.923702]\n",
      "******* 418 [D loss: 0.023909, acc: 100.00%] [G loss: 5.820940]\n",
      "******* 419 [D loss: 0.038414, acc: 98.44%] [G loss: 5.538133]\n",
      "******* 420 [D loss: 0.020378, acc: 100.00%] [G loss: 4.975150]\n",
      "******* 421 [D loss: 0.062012, acc: 99.22%] [G loss: 5.071488]\n",
      "******* 422 [D loss: 0.018983, acc: 100.00%] [G loss: 4.447975]\n",
      "******* 423 [D loss: 0.049175, acc: 99.22%] [G loss: 4.174993]\n",
      "******* 424 [D loss: 0.042427, acc: 99.22%] [G loss: 4.057607]\n",
      "******* 425 [D loss: 0.029140, acc: 100.00%] [G loss: 4.246483]\n",
      "******* 426 [D loss: 0.024367, acc: 100.00%] [G loss: 4.580629]\n",
      "******* 427 [D loss: 0.021459, acc: 100.00%] [G loss: 5.199708]\n",
      "******* 428 [D loss: 0.041557, acc: 99.22%] [G loss: 5.124057]\n",
      "******* 429 [D loss: 0.030716, acc: 100.00%] [G loss: 4.699059]\n",
      "******* 430 [D loss: 0.041976, acc: 99.22%] [G loss: 4.545845]\n",
      "******* 431 [D loss: 0.037540, acc: 100.00%] [G loss: 4.233560]\n",
      "******* 432 [D loss: 0.047842, acc: 100.00%] [G loss: 4.748638]\n",
      "******* 433 [D loss: 0.021728, acc: 100.00%] [G loss: 4.964077]\n",
      "******* 434 [D loss: 0.031070, acc: 99.22%] [G loss: 4.955302]\n",
      "******* 435 [D loss: 0.024094, acc: 99.22%] [G loss: 4.874610]\n",
      "******* 436 [D loss: 0.023068, acc: 100.00%] [G loss: 4.862094]\n",
      "******* 437 [D loss: 0.026363, acc: 100.00%] [G loss: 4.788570]\n",
      "******* 438 [D loss: 0.022948, acc: 100.00%] [G loss: 4.715802]\n",
      "******* 439 [D loss: 0.018379, acc: 100.00%] [G loss: 4.650052]\n",
      "******* 440 [D loss: 0.021760, acc: 100.00%] [G loss: 5.101661]\n",
      "******* 441 [D loss: 0.024052, acc: 100.00%] [G loss: 4.774118]\n",
      "******* 442 [D loss: 0.020361, acc: 100.00%] [G loss: 5.496783]\n",
      "******* 443 [D loss: 0.050471, acc: 98.44%] [G loss: 5.043727]\n",
      "******* 444 [D loss: 0.035141, acc: 99.22%] [G loss: 5.064281]\n",
      "******* 445 [D loss: 0.021445, acc: 99.22%] [G loss: 4.839159]\n",
      "******* 446 [D loss: 0.022501, acc: 100.00%] [G loss: 5.367826]\n",
      "******* 447 [D loss: 0.013882, acc: 100.00%] [G loss: 5.668461]\n",
      "******* 448 [D loss: 0.017239, acc: 100.00%] [G loss: 5.827021]\n",
      "******* 449 [D loss: 0.015078, acc: 100.00%] [G loss: 5.736825]\n",
      "******* 450 [D loss: 0.019682, acc: 100.00%] [G loss: 5.544202]\n",
      "******* 451 [D loss: 0.029740, acc: 99.22%] [G loss: 4.779740]\n",
      "******* 452 [D loss: 0.017434, acc: 100.00%] [G loss: 4.735767]\n",
      "******* 453 [D loss: 0.032594, acc: 99.22%] [G loss: 4.725485]\n",
      "******* 454 [D loss: 0.022119, acc: 100.00%] [G loss: 5.288160]\n",
      "******* 455 [D loss: 0.010386, acc: 100.00%] [G loss: 5.586072]\n",
      "******* 456 [D loss: 0.020953, acc: 100.00%] [G loss: 5.107951]\n",
      "******* 457 [D loss: 0.027992, acc: 100.00%] [G loss: 4.725992]\n",
      "******* 458 [D loss: 0.019267, acc: 100.00%] [G loss: 4.866582]\n",
      "******* 459 [D loss: 0.025212, acc: 100.00%] [G loss: 5.016298]\n",
      "******* 460 [D loss: 0.017385, acc: 100.00%] [G loss: 5.062627]\n",
      "******* 461 [D loss: 0.014615, acc: 100.00%] [G loss: 5.358431]\n",
      "******* 462 [D loss: 0.018815, acc: 99.22%] [G loss: 5.048141]\n",
      "******* 463 [D loss: 0.011886, acc: 100.00%] [G loss: 5.030937]\n",
      "******* 464 [D loss: 0.018473, acc: 100.00%] [G loss: 5.345627]\n",
      "******* 465 [D loss: 0.011655, acc: 100.00%] [G loss: 5.285829]\n",
      "******* 466 [D loss: 0.035243, acc: 100.00%] [G loss: 4.398000]\n",
      "******* 467 [D loss: 0.029175, acc: 100.00%] [G loss: 4.378524]\n",
      "******* 468 [D loss: 0.033294, acc: 100.00%] [G loss: 4.798067]\n",
      "******* 469 [D loss: 0.026462, acc: 100.00%] [G loss: 5.383272]\n",
      "******* 470 [D loss: 0.025052, acc: 99.22%] [G loss: 5.366062]\n",
      "******* 471 [D loss: 0.010314, acc: 100.00%] [G loss: 5.203447]\n",
      "******* 472 [D loss: 0.038860, acc: 99.22%] [G loss: 4.778067]\n",
      "******* 473 [D loss: 0.020178, acc: 100.00%] [G loss: 4.818091]\n",
      "******* 474 [D loss: 0.016164, acc: 100.00%] [G loss: 4.353391]\n",
      "******* 475 [D loss: 0.020866, acc: 100.00%] [G loss: 4.753140]\n",
      "******* 476 [D loss: 0.022118, acc: 100.00%] [G loss: 5.234206]\n",
      "******* 477 [D loss: 0.009472, acc: 100.00%] [G loss: 5.561602]\n",
      "******* 478 [D loss: 0.012635, acc: 100.00%] [G loss: 5.938480]\n",
      "******* 479 [D loss: 0.030792, acc: 99.22%] [G loss: 5.277691]\n",
      "******* 480 [D loss: 0.022951, acc: 100.00%] [G loss: 4.550983]\n",
      "******* 481 [D loss: 0.026463, acc: 100.00%] [G loss: 4.485533]\n",
      "******* 482 [D loss: 0.017327, acc: 100.00%] [G loss: 4.639921]\n",
      "******* 483 [D loss: 0.018776, acc: 100.00%] [G loss: 5.421844]\n",
      "******* 484 [D loss: 0.017734, acc: 100.00%] [G loss: 5.733446]\n",
      "******* 485 [D loss: 0.045055, acc: 99.22%] [G loss: 5.138485]\n",
      "******* 486 [D loss: 0.030822, acc: 99.22%] [G loss: 4.574433]\n",
      "******* 487 [D loss: 0.016846, acc: 100.00%] [G loss: 5.041490]\n",
      "******* 488 [D loss: 0.018462, acc: 100.00%] [G loss: 5.452684]\n",
      "******* 489 [D loss: 0.018630, acc: 100.00%] [G loss: 5.477493]\n",
      "******* 490 [D loss: 0.026898, acc: 100.00%] [G loss: 4.904732]\n",
      "******* 491 [D loss: 0.016164, acc: 100.00%] [G loss: 5.248763]\n",
      "******* 492 [D loss: 0.018073, acc: 100.00%] [G loss: 5.303350]\n",
      "******* 493 [D loss: 0.012734, acc: 100.00%] [G loss: 4.897703]\n",
      "******* 494 [D loss: 0.013188, acc: 100.00%] [G loss: 4.870772]\n",
      "******* 495 [D loss: 0.016832, acc: 100.00%] [G loss: 5.189883]\n",
      "******* 496 [D loss: 0.038470, acc: 99.22%] [G loss: 5.045440]\n",
      "******* 497 [D loss: 0.018861, acc: 100.00%] [G loss: 5.047622]\n",
      "******* 498 [D loss: 0.022187, acc: 100.00%] [G loss: 5.635253]\n",
      "******* 499 [D loss: 0.020870, acc: 99.22%] [G loss: 5.379856]\n",
      "******* 500 [D loss: 0.015641, acc: 100.00%] [G loss: 5.211322]\n",
      "******* 501 [D loss: 0.022486, acc: 99.22%] [G loss: 4.948764]\n",
      "******* 502 [D loss: 0.019448, acc: 100.00%] [G loss: 5.002152]\n",
      "******* 503 [D loss: 0.026135, acc: 100.00%] [G loss: 4.451198]\n",
      "******* 504 [D loss: 0.018797, acc: 100.00%] [G loss: 4.813233]\n",
      "******* 505 [D loss: 0.019367, acc: 100.00%] [G loss: 5.125690]\n",
      "******* 506 [D loss: 0.020119, acc: 100.00%] [G loss: 5.325654]\n",
      "******* 507 [D loss: 0.029793, acc: 100.00%] [G loss: 5.179171]\n",
      "******* 508 [D loss: 0.018514, acc: 100.00%] [G loss: 5.018008]\n",
      "******* 509 [D loss: 0.017189, acc: 100.00%] [G loss: 5.107149]\n",
      "******* 510 [D loss: 0.011604, acc: 100.00%] [G loss: 5.492193]\n",
      "******* 511 [D loss: 0.009253, acc: 100.00%] [G loss: 5.518795]\n",
      "******* 512 [D loss: 0.034834, acc: 98.44%] [G loss: 4.481382]\n",
      "******* 513 [D loss: 0.020638, acc: 100.00%] [G loss: 4.257874]\n",
      "******* 514 [D loss: 0.016695, acc: 100.00%] [G loss: 4.503420]\n",
      "******* 515 [D loss: 0.023317, acc: 100.00%] [G loss: 4.613523]\n",
      "******* 516 [D loss: 0.009716, acc: 100.00%] [G loss: 5.141994]\n",
      "******* 517 [D loss: 0.011998, acc: 100.00%] [G loss: 5.463943]\n",
      "******* 518 [D loss: 0.013130, acc: 100.00%] [G loss: 5.126120]\n",
      "******* 519 [D loss: 0.016449, acc: 100.00%] [G loss: 4.846522]\n",
      "******* 520 [D loss: 0.027774, acc: 100.00%] [G loss: 4.298055]\n",
      "******* 521 [D loss: 0.020974, acc: 100.00%] [G loss: 4.322497]\n",
      "******* 522 [D loss: 0.027062, acc: 100.00%] [G loss: 4.982498]\n",
      "******* 523 [D loss: 0.018289, acc: 100.00%] [G loss: 5.348417]\n",
      "******* 524 [D loss: 0.022257, acc: 99.22%] [G loss: 5.036252]\n",
      "******* 525 [D loss: 0.032211, acc: 100.00%] [G loss: 4.107821]\n",
      "******* 526 [D loss: 0.025260, acc: 100.00%] [G loss: 3.955878]\n",
      "******* 527 [D loss: 0.028884, acc: 100.00%] [G loss: 4.532512]\n",
      "******* 528 [D loss: 0.009036, acc: 100.00%] [G loss: 5.285304]\n",
      "******* 529 [D loss: 0.011123, acc: 100.00%] [G loss: 6.024597]\n",
      "******* 530 [D loss: 0.013557, acc: 100.00%] [G loss: 5.701892]\n",
      "******* 531 [D loss: 0.009717, acc: 100.00%] [G loss: 5.520917]\n",
      "******* 532 [D loss: 0.010853, acc: 100.00%] [G loss: 5.494753]\n",
      "******* 533 [D loss: 0.012509, acc: 100.00%] [G loss: 4.901118]\n",
      "******* 534 [D loss: 0.016638, acc: 100.00%] [G loss: 5.326417]\n",
      "******* 535 [D loss: 0.012648, acc: 100.00%] [G loss: 4.726882]\n",
      "******* 536 [D loss: 0.017046, acc: 100.00%] [G loss: 5.034656]\n",
      "******* 537 [D loss: 0.027453, acc: 100.00%] [G loss: 4.955845]\n",
      "******* 538 [D loss: 0.017059, acc: 100.00%] [G loss: 4.550741]\n",
      "******* 539 [D loss: 0.023521, acc: 100.00%] [G loss: 4.745071]\n",
      "******* 540 [D loss: 0.022806, acc: 100.00%] [G loss: 5.234313]\n",
      "******* 541 [D loss: 0.024411, acc: 100.00%] [G loss: 4.974000]\n",
      "******* 542 [D loss: 0.029481, acc: 100.00%] [G loss: 4.946906]\n",
      "******* 543 [D loss: 0.027137, acc: 99.22%] [G loss: 4.741825]\n",
      "******* 544 [D loss: 0.035027, acc: 100.00%] [G loss: 4.436895]\n",
      "******* 545 [D loss: 0.030249, acc: 99.22%] [G loss: 4.969215]\n",
      "******* 546 [D loss: 0.015322, acc: 100.00%] [G loss: 5.511986]\n",
      "******* 547 [D loss: 0.010458, acc: 100.00%] [G loss: 5.850624]\n",
      "******* 548 [D loss: 0.027082, acc: 99.22%] [G loss: 5.310565]\n",
      "******* 549 [D loss: 0.018513, acc: 100.00%] [G loss: 5.249019]\n",
      "******* 550 [D loss: 0.012722, acc: 100.00%] [G loss: 4.875103]\n",
      "******* 551 [D loss: 0.014983, acc: 100.00%] [G loss: 5.071845]\n",
      "******* 552 [D loss: 0.012847, acc: 100.00%] [G loss: 5.375164]\n",
      "******* 553 [D loss: 0.011804, acc: 100.00%] [G loss: 5.625419]\n",
      "******* 554 [D loss: 0.015786, acc: 100.00%] [G loss: 5.474120]\n",
      "******* 555 [D loss: 0.034588, acc: 100.00%] [G loss: 4.585114]\n",
      "******* 556 [D loss: 0.023674, acc: 100.00%] [G loss: 4.346214]\n",
      "******* 557 [D loss: 0.022809, acc: 100.00%] [G loss: 4.778388]\n",
      "******* 558 [D loss: 0.021707, acc: 100.00%] [G loss: 5.412773]\n",
      "******* 559 [D loss: 0.023908, acc: 100.00%] [G loss: 5.075757]\n",
      "******* 560 [D loss: 0.032765, acc: 99.22%] [G loss: 4.902516]\n",
      "******* 561 [D loss: 0.040160, acc: 100.00%] [G loss: 4.409579]\n",
      "******* 562 [D loss: 0.021228, acc: 100.00%] [G loss: 4.841685]\n",
      "******* 563 [D loss: 0.013001, acc: 100.00%] [G loss: 5.274260]\n",
      "******* 564 [D loss: 0.014747, acc: 100.00%] [G loss: 5.500126]\n",
      "******* 565 [D loss: 0.031877, acc: 99.22%] [G loss: 5.074285]\n",
      "******* 566 [D loss: 0.014293, acc: 100.00%] [G loss: 5.272612]\n",
      "******* 567 [D loss: 0.021039, acc: 100.00%] [G loss: 4.802709]\n",
      "******* 568 [D loss: 0.056813, acc: 99.22%] [G loss: 4.487839]\n",
      "******* 569 [D loss: 0.031563, acc: 99.22%] [G loss: 4.448497]\n",
      "******* 570 [D loss: 0.012885, acc: 100.00%] [G loss: 5.464806]\n",
      "******* 571 [D loss: 0.057440, acc: 99.22%] [G loss: 4.948082]\n",
      "******* 572 [D loss: 0.040292, acc: 100.00%] [G loss: 5.061317]\n",
      "******* 573 [D loss: 0.017737, acc: 100.00%] [G loss: 5.407376]\n",
      "******* 574 [D loss: 0.021262, acc: 100.00%] [G loss: 5.562079]\n",
      "******* 575 [D loss: 0.019941, acc: 100.00%] [G loss: 4.461868]\n",
      "******* 576 [D loss: 0.038726, acc: 100.00%] [G loss: 4.574473]\n",
      "******* 577 [D loss: 0.023682, acc: 100.00%] [G loss: 5.169430]\n",
      "******* 578 [D loss: 0.014040, acc: 100.00%] [G loss: 5.518434]\n",
      "******* 579 [D loss: 0.040721, acc: 99.22%] [G loss: 5.243925]\n",
      "******* 580 [D loss: 0.025533, acc: 99.22%] [G loss: 4.076832]\n",
      "******* 581 [D loss: 0.037459, acc: 100.00%] [G loss: 4.146995]\n",
      "******* 582 [D loss: 0.022664, acc: 100.00%] [G loss: 5.422240]\n",
      "******* 583 [D loss: 0.016662, acc: 100.00%] [G loss: 6.034993]\n",
      "******* 584 [D loss: 0.041146, acc: 99.22%] [G loss: 4.523168]\n",
      "******* 585 [D loss: 0.085913, acc: 96.88%] [G loss: 4.408868]\n",
      "******* 586 [D loss: 0.031984, acc: 100.00%] [G loss: 4.364352]\n",
      "******* 587 [D loss: 0.021017, acc: 100.00%] [G loss: 6.201699]\n",
      "******* 588 [D loss: 0.048188, acc: 99.22%] [G loss: 5.587661]\n",
      "******* 589 [D loss: 0.049881, acc: 98.44%] [G loss: 4.465657]\n",
      "******* 590 [D loss: 0.031163, acc: 100.00%] [G loss: 4.594195]\n",
      "******* 591 [D loss: 0.032479, acc: 99.22%] [G loss: 4.072510]\n",
      "******* 592 [D loss: 0.025843, acc: 100.00%] [G loss: 5.261925]\n",
      "******* 593 [D loss: 0.018577, acc: 100.00%] [G loss: 6.400849]\n",
      "******* 594 [D loss: 0.027749, acc: 99.22%] [G loss: 6.108670]\n",
      "******* 595 [D loss: 0.012957, acc: 100.00%] [G loss: 6.165407]\n",
      "******* 596 [D loss: 0.024888, acc: 99.22%] [G loss: 5.128041]\n",
      "******* 597 [D loss: 0.029535, acc: 100.00%] [G loss: 5.545223]\n",
      "******* 598 [D loss: 0.015114, acc: 100.00%] [G loss: 4.793000]\n",
      "******* 599 [D loss: 0.018766, acc: 100.00%] [G loss: 5.473918]\n",
      "******* 600 [D loss: 0.042944, acc: 99.22%] [G loss: 4.854163]\n",
      "0.00000004\n",
      "saved\n",
      "******* 601 [D loss: 0.029902, acc: 100.00%] [G loss: 4.625882]\n",
      "******* 602 [D loss: 0.031138, acc: 100.00%] [G loss: 4.423346]\n",
      "******* 603 [D loss: 0.022906, acc: 100.00%] [G loss: 4.839418]\n",
      "******* 604 [D loss: 0.024025, acc: 100.00%] [G loss: 4.982186]\n",
      "******* 605 [D loss: 0.064507, acc: 98.44%] [G loss: 3.941056]\n",
      "******* 606 [D loss: 0.077083, acc: 99.22%] [G loss: 4.502957]\n",
      "******* 607 [D loss: 0.027412, acc: 100.00%] [G loss: 5.958642]\n",
      "******* 608 [D loss: 0.029673, acc: 99.22%] [G loss: 5.860042]\n",
      "******* 609 [D loss: 0.023275, acc: 99.22%] [G loss: 5.557702]\n",
      "******* 610 [D loss: 0.028382, acc: 99.22%] [G loss: 4.949704]\n",
      "******* 611 [D loss: 0.031153, acc: 99.22%] [G loss: 4.912558]\n",
      "******* 612 [D loss: 0.017940, acc: 100.00%] [G loss: 5.226617]\n",
      "******* 613 [D loss: 0.038776, acc: 99.22%] [G loss: 4.908607]\n",
      "******* 614 [D loss: 0.017603, acc: 100.00%] [G loss: 5.040282]\n",
      "******* 615 [D loss: 0.017375, acc: 100.00%] [G loss: 4.992714]\n",
      "******* 616 [D loss: 0.043548, acc: 99.22%] [G loss: 4.207052]\n",
      "******* 617 [D loss: 0.034543, acc: 99.22%] [G loss: 4.657088]\n",
      "******* 618 [D loss: 0.026711, acc: 100.00%] [G loss: 5.078687]\n",
      "******* 619 [D loss: 0.117274, acc: 96.88%] [G loss: 3.407525]\n",
      "******* 620 [D loss: 0.065994, acc: 100.00%] [G loss: 3.990038]\n",
      "******* 621 [D loss: 0.019706, acc: 100.00%] [G loss: 5.864381]\n",
      "******* 622 [D loss: 0.028644, acc: 99.22%] [G loss: 6.575341]\n",
      "******* 623 [D loss: 0.056759, acc: 97.66%] [G loss: 4.945944]\n",
      "******* 624 [D loss: 0.035722, acc: 99.22%] [G loss: 3.802236]\n",
      "******* 625 [D loss: 0.027692, acc: 100.00%] [G loss: 4.112221]\n",
      "******* 626 [D loss: 0.021677, acc: 100.00%] [G loss: 5.236762]\n",
      "******* 627 [D loss: 0.007401, acc: 100.00%] [G loss: 6.827511]\n",
      "******* 628 [D loss: 0.066998, acc: 98.44%] [G loss: 5.372596]\n",
      "******* 629 [D loss: 0.042450, acc: 98.44%] [G loss: 3.754856]\n",
      "******* 630 [D loss: 0.033153, acc: 100.00%] [G loss: 4.017112]\n",
      "******* 631 [D loss: 0.030655, acc: 100.00%] [G loss: 5.222183]\n",
      "******* 632 [D loss: 0.033297, acc: 98.44%] [G loss: 5.949549]\n",
      "******* 633 [D loss: 0.114193, acc: 96.88%] [G loss: 3.675633]\n",
      "******* 634 [D loss: 0.115774, acc: 97.66%] [G loss: 4.185217]\n",
      "******* 635 [D loss: 0.015636, acc: 100.00%] [G loss: 6.290591]\n",
      "******* 636 [D loss: 0.042393, acc: 99.22%] [G loss: 6.322443]\n",
      "******* 637 [D loss: 0.030752, acc: 99.22%] [G loss: 6.384630]\n",
      "******* 638 [D loss: 0.076782, acc: 96.09%] [G loss: 4.783177]\n",
      "******* 639 [D loss: 0.041527, acc: 100.00%] [G loss: 4.021586]\n",
      "******* 640 [D loss: 0.032903, acc: 100.00%] [G loss: 4.848171]\n",
      "******* 641 [D loss: 0.013326, acc: 100.00%] [G loss: 6.529363]\n",
      "******* 642 [D loss: 0.015386, acc: 99.22%] [G loss: 6.863246]\n",
      "******* 643 [D loss: 0.089402, acc: 97.66%] [G loss: 3.904063]\n",
      "******* 644 [D loss: 0.101968, acc: 96.88%] [G loss: 3.405566]\n",
      "******* 645 [D loss: 0.039506, acc: 99.22%] [G loss: 4.886926]\n",
      "******* 646 [D loss: 0.017168, acc: 100.00%] [G loss: 6.726823]\n",
      "******* 647 [D loss: 0.020075, acc: 100.00%] [G loss: 6.625139]\n",
      "******* 648 [D loss: 0.037362, acc: 98.44%] [G loss: 5.058924]\n",
      "******* 649 [D loss: 0.044246, acc: 99.22%] [G loss: 3.948533]\n",
      "******* 650 [D loss: 0.070403, acc: 99.22%] [G loss: 4.998911]\n",
      "******* 651 [D loss: 0.026702, acc: 100.00%] [G loss: 6.075614]\n",
      "******* 652 [D loss: 0.037993, acc: 98.44%] [G loss: 5.477488]\n",
      "******* 653 [D loss: 0.035617, acc: 99.22%] [G loss: 4.464640]\n",
      "******* 654 [D loss: 0.053049, acc: 99.22%] [G loss: 4.219646]\n",
      "******* 655 [D loss: 0.027375, acc: 100.00%] [G loss: 4.327039]\n",
      "******* 656 [D loss: 0.019399, acc: 100.00%] [G loss: 5.316997]\n",
      "******* 657 [D loss: 0.049529, acc: 99.22%] [G loss: 4.738305]\n",
      "******* 658 [D loss: 0.050388, acc: 99.22%] [G loss: 4.392972]\n",
      "******* 659 [D loss: 0.061083, acc: 99.22%] [G loss: 4.533255]\n",
      "******* 660 [D loss: 0.066481, acc: 98.44%] [G loss: 4.644539]\n",
      "******* 661 [D loss: 0.087431, acc: 98.44%] [G loss: 4.532752]\n",
      "******* 662 [D loss: 0.039590, acc: 100.00%] [G loss: 4.078251]\n",
      "******* 663 [D loss: 0.039088, acc: 100.00%] [G loss: 4.075770]\n",
      "******* 664 [D loss: 0.048354, acc: 99.22%] [G loss: 5.268388]\n",
      "******* 665 [D loss: 0.044629, acc: 100.00%] [G loss: 4.708328]\n",
      "******* 666 [D loss: 0.063712, acc: 98.44%] [G loss: 4.479899]\n",
      "******* 667 [D loss: 0.117578, acc: 96.09%] [G loss: 3.617428]\n",
      "******* 668 [D loss: 0.072611, acc: 98.44%] [G loss: 4.592853]\n",
      "******* 669 [D loss: 0.030586, acc: 98.44%] [G loss: 5.848576]\n",
      "******* 670 [D loss: 0.077493, acc: 97.66%] [G loss: 4.786283]\n",
      "******* 671 [D loss: 0.043166, acc: 99.22%] [G loss: 3.979265]\n",
      "******* 672 [D loss: 0.066156, acc: 98.44%] [G loss: 4.738636]\n",
      "******* 673 [D loss: 0.018443, acc: 100.00%] [G loss: 6.361570]\n",
      "******* 674 [D loss: 0.022678, acc: 99.22%] [G loss: 6.343691]\n",
      "******* 675 [D loss: 0.082730, acc: 98.44%] [G loss: 5.257936]\n",
      "******* 676 [D loss: 0.075978, acc: 97.66%] [G loss: 4.122106]\n",
      "******* 677 [D loss: 0.074414, acc: 100.00%] [G loss: 4.241775]\n",
      "******* 678 [D loss: 0.016204, acc: 100.00%] [G loss: 5.954918]\n",
      "******* 679 [D loss: 0.076085, acc: 98.44%] [G loss: 5.358742]\n",
      "******* 680 [D loss: 0.042868, acc: 98.44%] [G loss: 4.768528]\n",
      "******* 681 [D loss: 0.086217, acc: 96.88%] [G loss: 3.843012]\n",
      "******* 682 [D loss: 0.064491, acc: 99.22%] [G loss: 3.750992]\n",
      "******* 683 [D loss: 0.083470, acc: 96.88%] [G loss: 4.862602]\n",
      "******* 684 [D loss: 0.093877, acc: 98.44%] [G loss: 4.443210]\n",
      "******* 685 [D loss: 0.039035, acc: 99.22%] [G loss: 5.407795]\n",
      "******* 686 [D loss: 0.012606, acc: 100.00%] [G loss: 5.630154]\n",
      "******* 687 [D loss: 0.073514, acc: 96.88%] [G loss: 4.068528]\n",
      "******* 688 [D loss: 0.041364, acc: 100.00%] [G loss: 3.985389]\n",
      "******* 689 [D loss: 0.039252, acc: 100.00%] [G loss: 4.352022]\n",
      "******* 690 [D loss: 0.056782, acc: 99.22%] [G loss: 4.714746]\n",
      "******* 691 [D loss: 0.067668, acc: 100.00%] [G loss: 4.297608]\n",
      "******* 692 [D loss: 0.070550, acc: 99.22%] [G loss: 4.209152]\n",
      "******* 693 [D loss: 0.024538, acc: 100.00%] [G loss: 5.454020]\n",
      "******* 694 [D loss: 0.056491, acc: 98.44%] [G loss: 4.672690]\n",
      "******* 695 [D loss: 0.048138, acc: 100.00%] [G loss: 4.346007]\n",
      "******* 696 [D loss: 0.060326, acc: 100.00%] [G loss: 4.437871]\n",
      "******* 697 [D loss: 0.020870, acc: 100.00%] [G loss: 5.899380]\n",
      "******* 698 [D loss: 0.213658, acc: 92.97%] [G loss: 3.104416]\n",
      "******* 699 [D loss: 0.160125, acc: 93.75%] [G loss: 3.911286]\n",
      "******* 700 [D loss: 0.022110, acc: 100.00%] [G loss: 6.960598]\n",
      "******* 701 [D loss: 0.046047, acc: 98.44%] [G loss: 7.350959]\n",
      "******* 702 [D loss: 0.091843, acc: 96.09%] [G loss: 4.280820]\n",
      "******* 703 [D loss: 0.176197, acc: 91.41%] [G loss: 4.562637]\n",
      "******* 704 [D loss: 0.024636, acc: 100.00%] [G loss: 6.863974]\n",
      "******* 705 [D loss: 0.050845, acc: 97.66%] [G loss: 7.431840]\n",
      "******* 706 [D loss: 0.051448, acc: 99.22%] [G loss: 5.779211]\n",
      "******* 707 [D loss: 0.060976, acc: 98.44%] [G loss: 4.637558]\n",
      "******* 708 [D loss: 0.093474, acc: 97.66%] [G loss: 4.014224]\n",
      "******* 709 [D loss: 0.028145, acc: 100.00%] [G loss: 4.041749]\n",
      "******* 710 [D loss: 0.047026, acc: 99.22%] [G loss: 4.760530]\n",
      "******* 711 [D loss: 0.037965, acc: 99.22%] [G loss: 5.343531]\n",
      "******* 712 [D loss: 0.018904, acc: 100.00%] [G loss: 5.346232]\n",
      "******* 713 [D loss: 0.160406, acc: 96.09%] [G loss: 3.816185]\n",
      "******* 714 [D loss: 0.140142, acc: 97.66%] [G loss: 4.636065]\n",
      "******* 715 [D loss: 0.033006, acc: 100.00%] [G loss: 6.330884]\n",
      "******* 716 [D loss: 0.156123, acc: 94.53%] [G loss: 3.745409]\n",
      "******* 717 [D loss: 0.147638, acc: 96.88%] [G loss: 3.606256]\n",
      "******* 718 [D loss: 0.071483, acc: 98.44%] [G loss: 4.759152]\n",
      "******* 719 [D loss: 0.021325, acc: 99.22%] [G loss: 6.368752]\n",
      "******* 720 [D loss: 0.141130, acc: 96.88%] [G loss: 5.203721]\n",
      "******* 721 [D loss: 0.061104, acc: 99.22%] [G loss: 4.392707]\n",
      "******* 722 [D loss: 0.035485, acc: 99.22%] [G loss: 4.431304]\n",
      "******* 723 [D loss: 0.035409, acc: 100.00%] [G loss: 4.702844]\n",
      "******* 724 [D loss: 0.039245, acc: 100.00%] [G loss: 5.599811]\n",
      "******* 725 [D loss: 0.034307, acc: 100.00%] [G loss: 5.625669]\n",
      "******* 726 [D loss: 0.075769, acc: 98.44%] [G loss: 4.515268]\n",
      "******* 727 [D loss: 0.097702, acc: 98.44%] [G loss: 3.756441]\n",
      "******* 728 [D loss: 0.037197, acc: 99.22%] [G loss: 4.704321]\n",
      "******* 729 [D loss: 0.028246, acc: 100.00%] [G loss: 5.769239]\n",
      "******* 730 [D loss: 0.054623, acc: 98.44%] [G loss: 5.037787]\n",
      "******* 731 [D loss: 0.068306, acc: 99.22%] [G loss: 3.835823]\n",
      "******* 732 [D loss: 0.081736, acc: 99.22%] [G loss: 4.043372]\n",
      "******* 733 [D loss: 0.021746, acc: 100.00%] [G loss: 5.251760]\n",
      "******* 734 [D loss: 0.073336, acc: 97.66%] [G loss: 4.456941]\n",
      "******* 735 [D loss: 0.089579, acc: 97.66%] [G loss: 3.632547]\n",
      "******* 736 [D loss: 0.042539, acc: 99.22%] [G loss: 4.732043]\n",
      "******* 737 [D loss: 0.093208, acc: 96.09%] [G loss: 4.731579]\n",
      "******* 738 [D loss: 0.067147, acc: 98.44%] [G loss: 4.140949]\n",
      "******* 739 [D loss: 0.046795, acc: 99.22%] [G loss: 3.757409]\n",
      "******* 740 [D loss: 0.028958, acc: 100.00%] [G loss: 4.442081]\n",
      "******* 741 [D loss: 0.112452, acc: 96.09%] [G loss: 3.998885]\n",
      "******* 742 [D loss: 0.104021, acc: 98.44%] [G loss: 4.620708]\n",
      "******* 743 [D loss: 0.024040, acc: 100.00%] [G loss: 6.076760]\n",
      "******* 744 [D loss: 0.140229, acc: 95.31%] [G loss: 4.039291]\n",
      "******* 745 [D loss: 0.136195, acc: 94.53%] [G loss: 4.213799]\n",
      "******* 746 [D loss: 0.075602, acc: 98.44%] [G loss: 5.370555]\n",
      "******* 747 [D loss: 0.063668, acc: 98.44%] [G loss: 5.424274]\n",
      "******* 748 [D loss: 0.068980, acc: 99.22%] [G loss: 4.460279]\n",
      "******* 749 [D loss: 0.063026, acc: 99.22%] [G loss: 3.723846]\n",
      "******* 750 [D loss: 0.050174, acc: 99.22%] [G loss: 4.250596]\n",
      "******* 751 [D loss: 0.027117, acc: 100.00%] [G loss: 5.700304]\n",
      "******* 752 [D loss: 0.058078, acc: 97.66%] [G loss: 5.565457]\n",
      "******* 753 [D loss: 0.041789, acc: 99.22%] [G loss: 5.125132]\n",
      "******* 754 [D loss: 0.117301, acc: 96.88%] [G loss: 3.517809]\n",
      "******* 755 [D loss: 0.052211, acc: 100.00%] [G loss: 4.820802]\n",
      "******* 756 [D loss: 0.070794, acc: 99.22%] [G loss: 5.563323]\n",
      "******* 757 [D loss: 0.168911, acc: 95.31%] [G loss: 3.672986]\n",
      "******* 758 [D loss: 0.180126, acc: 93.75%] [G loss: 3.655452]\n",
      "******* 759 [D loss: 0.047550, acc: 99.22%] [G loss: 6.051943]\n",
      "******* 760 [D loss: 0.153007, acc: 95.31%] [G loss: 4.559893]\n",
      "******* 761 [D loss: 0.093137, acc: 97.66%] [G loss: 3.343130]\n",
      "******* 762 [D loss: 0.088027, acc: 98.44%] [G loss: 4.460913]\n",
      "******* 763 [D loss: 0.058122, acc: 98.44%] [G loss: 5.335873]\n",
      "******* 764 [D loss: 0.174560, acc: 92.97%] [G loss: 3.650813]\n",
      "******* 765 [D loss: 0.078408, acc: 98.44%] [G loss: 3.945405]\n",
      "******* 766 [D loss: 0.034845, acc: 100.00%] [G loss: 4.874229]\n",
      "******* 767 [D loss: 0.052371, acc: 98.44%] [G loss: 5.266820]\n",
      "******* 768 [D loss: 0.128857, acc: 96.88%] [G loss: 2.949744]\n",
      "******* 769 [D loss: 0.115709, acc: 95.31%] [G loss: 4.567252]\n",
      "******* 770 [D loss: 0.059551, acc: 97.66%] [G loss: 5.421438]\n",
      "******* 771 [D loss: 0.117935, acc: 96.09%] [G loss: 3.905240]\n",
      "******* 772 [D loss: 0.089896, acc: 100.00%] [G loss: 3.616590]\n",
      "******* 773 [D loss: 0.076484, acc: 98.44%] [G loss: 4.201459]\n",
      "******* 774 [D loss: 0.125744, acc: 96.88%] [G loss: 4.429635]\n",
      "******* 775 [D loss: 0.079180, acc: 99.22%] [G loss: 4.505962]\n",
      "******* 776 [D loss: 0.057883, acc: 97.66%] [G loss: 4.271309]\n",
      "******* 777 [D loss: 0.044565, acc: 100.00%] [G loss: 4.453942]\n",
      "******* 778 [D loss: 0.034080, acc: 99.22%] [G loss: 4.111732]\n",
      "******* 779 [D loss: 0.068203, acc: 98.44%] [G loss: 3.349613]\n",
      "******* 780 [D loss: 0.074379, acc: 99.22%] [G loss: 4.234967]\n",
      "******* 781 [D loss: 0.075375, acc: 96.88%] [G loss: 4.294331]\n",
      "******* 782 [D loss: 0.115692, acc: 97.66%] [G loss: 3.481418]\n",
      "******* 783 [D loss: 0.266429, acc: 87.50%] [G loss: 4.290515]\n",
      "******* 784 [D loss: 0.064697, acc: 98.44%] [G loss: 5.790540]\n",
      "******* 785 [D loss: 0.239158, acc: 92.19%] [G loss: 2.527876]\n",
      "******* 786 [D loss: 0.321498, acc: 82.81%] [G loss: 3.598310]\n",
      "******* 787 [D loss: 0.051907, acc: 98.44%] [G loss: 7.372466]\n",
      "******* 788 [D loss: 0.252403, acc: 86.72%] [G loss: 3.657965]\n",
      "******* 789 [D loss: 0.281179, acc: 86.72%] [G loss: 3.434888]\n",
      "******* 790 [D loss: 0.027851, acc: 99.22%] [G loss: 6.060824]\n",
      "******* 791 [D loss: 0.115799, acc: 96.09%] [G loss: 6.137224]\n",
      "******* 792 [D loss: 0.105439, acc: 96.09%] [G loss: 4.568720]\n",
      "******* 793 [D loss: 0.061155, acc: 99.22%] [G loss: 3.651025]\n",
      "******* 794 [D loss: 0.085194, acc: 99.22%] [G loss: 5.170104]\n",
      "******* 795 [D loss: 0.071025, acc: 97.66%] [G loss: 5.437514]\n",
      "******* 796 [D loss: 0.058564, acc: 97.66%] [G loss: 4.601441]\n",
      "******* 797 [D loss: 0.129370, acc: 96.88%] [G loss: 3.374811]\n",
      "******* 798 [D loss: 0.131345, acc: 97.66%] [G loss: 3.646094]\n",
      "******* 799 [D loss: 0.058139, acc: 99.22%] [G loss: 5.224382]\n",
      "******* 800 [D loss: 0.222877, acc: 92.19%] [G loss: 3.832805]\n",
      "0.00000005\n",
      "saved\n",
      "******* 801 [D loss: 0.231121, acc: 89.84%] [G loss: 4.141625]\n",
      "******* 802 [D loss: 0.078317, acc: 98.44%] [G loss: 5.498537]\n",
      "******* 803 [D loss: 0.255010, acc: 91.41%] [G loss: 3.197964]\n",
      "******* 804 [D loss: 0.286383, acc: 85.94%] [G loss: 3.635890]\n",
      "******* 805 [D loss: 0.173262, acc: 96.88%] [G loss: 5.062882]\n",
      "******* 806 [D loss: 0.160651, acc: 94.53%] [G loss: 3.767409]\n",
      "******* 807 [D loss: 0.129221, acc: 96.88%] [G loss: 3.635562]\n",
      "******* 808 [D loss: 0.079486, acc: 99.22%] [G loss: 4.853549]\n",
      "******* 809 [D loss: 0.060610, acc: 97.66%] [G loss: 5.117067]\n",
      "******* 810 [D loss: 0.128658, acc: 97.66%] [G loss: 3.815878]\n",
      "******* 811 [D loss: 0.234027, acc: 92.97%] [G loss: 3.329947]\n",
      "******* 812 [D loss: 0.057911, acc: 99.22%] [G loss: 5.316924]\n",
      "******* 813 [D loss: 0.085006, acc: 97.66%] [G loss: 5.644744]\n",
      "******* 814 [D loss: 0.143760, acc: 94.53%] [G loss: 3.619060]\n",
      "******* 815 [D loss: 0.168004, acc: 93.75%] [G loss: 3.903874]\n",
      "******* 816 [D loss: 0.078864, acc: 98.44%] [G loss: 5.066859]\n",
      "******* 817 [D loss: 0.111856, acc: 96.88%] [G loss: 4.546564]\n",
      "******* 818 [D loss: 0.110528, acc: 96.09%] [G loss: 4.334830]\n",
      "******* 819 [D loss: 0.083834, acc: 97.66%] [G loss: 4.472396]\n",
      "******* 820 [D loss: 0.109775, acc: 96.09%] [G loss: 3.806890]\n",
      "******* 821 [D loss: 0.190545, acc: 94.53%] [G loss: 3.442645]\n",
      "******* 822 [D loss: 0.112656, acc: 98.44%] [G loss: 4.876162]\n",
      "******* 823 [D loss: 0.147151, acc: 94.53%] [G loss: 3.340365]\n",
      "******* 824 [D loss: 0.078608, acc: 98.44%] [G loss: 3.462403]\n",
      "******* 825 [D loss: 0.209685, acc: 92.19%] [G loss: 3.379666]\n",
      "******* 826 [D loss: 0.100182, acc: 97.66%] [G loss: 4.206627]\n",
      "******* 827 [D loss: 0.087857, acc: 98.44%] [G loss: 3.999918]\n",
      "******* 828 [D loss: 0.093281, acc: 97.66%] [G loss: 3.671869]\n",
      "******* 829 [D loss: 0.219809, acc: 93.75%] [G loss: 3.280565]\n",
      "******* 830 [D loss: 0.042184, acc: 100.00%] [G loss: 4.715544]\n",
      "******* 831 [D loss: 0.223094, acc: 91.41%] [G loss: 2.403400]\n",
      "******* 832 [D loss: 0.230254, acc: 91.41%] [G loss: 4.216138]\n",
      "******* 833 [D loss: 0.058779, acc: 97.66%] [G loss: 6.370096]\n",
      "******* 834 [D loss: 0.440615, acc: 84.38%] [G loss: 2.358440]\n",
      "******* 835 [D loss: 0.573619, acc: 66.41%] [G loss: 3.615921]\n",
      "******* 836 [D loss: 0.020446, acc: 100.00%] [G loss: 8.381779]\n",
      "******* 837 [D loss: 0.384244, acc: 84.38%] [G loss: 5.652469]\n",
      "******* 838 [D loss: 0.098708, acc: 96.09%] [G loss: 3.526634]\n",
      "******* 839 [D loss: 0.199264, acc: 89.06%] [G loss: 4.202791]\n",
      "******* 840 [D loss: 0.083312, acc: 97.66%] [G loss: 5.615304]\n",
      "******* 841 [D loss: 0.047393, acc: 97.66%] [G loss: 6.343894]\n",
      "******* 842 [D loss: 0.188303, acc: 93.75%] [G loss: 4.560114]\n",
      "******* 843 [D loss: 0.109365, acc: 96.09%] [G loss: 3.450880]\n",
      "******* 844 [D loss: 0.162886, acc: 92.19%] [G loss: 4.527841]\n",
      "******* 845 [D loss: 0.104569, acc: 96.88%] [G loss: 5.818182]\n",
      "******* 846 [D loss: 0.156668, acc: 92.97%] [G loss: 4.208338]\n",
      "******* 847 [D loss: 0.159223, acc: 95.31%] [G loss: 3.836159]\n",
      "******* 848 [D loss: 0.191268, acc: 94.53%] [G loss: 3.619102]\n",
      "******* 849 [D loss: 0.141689, acc: 96.09%] [G loss: 3.521548]\n",
      "******* 850 [D loss: 0.296013, acc: 86.72%] [G loss: 3.421033]\n",
      "******* 851 [D loss: 0.112633, acc: 97.66%] [G loss: 4.288156]\n",
      "******* 852 [D loss: 0.203005, acc: 94.53%] [G loss: 3.331756]\n",
      "******* 853 [D loss: 0.220808, acc: 94.53%] [G loss: 3.385811]\n",
      "******* 854 [D loss: 0.111985, acc: 98.44%] [G loss: 3.474707]\n",
      "******* 855 [D loss: 0.090652, acc: 99.22%] [G loss: 3.732976]\n",
      "******* 856 [D loss: 0.129115, acc: 96.88%] [G loss: 3.834592]\n",
      "******* 857 [D loss: 0.162253, acc: 96.09%] [G loss: 3.171206]\n",
      "******* 858 [D loss: 0.149349, acc: 96.88%] [G loss: 3.337721]\n",
      "******* 859 [D loss: 0.076088, acc: 99.22%] [G loss: 4.152329]\n",
      "******* 860 [D loss: 0.170613, acc: 96.09%] [G loss: 3.630354]\n",
      "******* 861 [D loss: 0.124113, acc: 99.22%] [G loss: 3.079781]\n",
      "******* 862 [D loss: 0.246926, acc: 90.62%] [G loss: 3.178976]\n",
      "******* 863 [D loss: 0.144859, acc: 96.09%] [G loss: 4.362222]\n",
      "******* 864 [D loss: 0.095762, acc: 98.44%] [G loss: 4.546591]\n",
      "******* 865 [D loss: 0.251168, acc: 91.41%] [G loss: 2.856486]\n",
      "******* 866 [D loss: 0.203746, acc: 94.53%] [G loss: 3.721176]\n",
      "******* 867 [D loss: 0.127648, acc: 94.53%] [G loss: 5.324461]\n",
      "******* 868 [D loss: 0.186137, acc: 92.97%] [G loss: 3.178958]\n",
      "******* 869 [D loss: 0.134357, acc: 96.09%] [G loss: 3.156497]\n",
      "******* 870 [D loss: 0.061625, acc: 98.44%] [G loss: 4.656056]\n",
      "******* 871 [D loss: 0.064896, acc: 98.44%] [G loss: 4.635222]\n",
      "******* 872 [D loss: 0.115359, acc: 96.88%] [G loss: 3.418233]\n",
      "******* 873 [D loss: 0.150679, acc: 96.09%] [G loss: 3.238615]\n",
      "******* 874 [D loss: 0.053806, acc: 100.00%] [G loss: 4.673834]\n",
      "******* 875 [D loss: 0.134418, acc: 98.44%] [G loss: 4.520107]\n",
      "******* 876 [D loss: 0.216859, acc: 92.97%] [G loss: 2.907264]\n",
      "******* 877 [D loss: 0.196706, acc: 92.97%] [G loss: 3.643406]\n",
      "******* 878 [D loss: 0.055810, acc: 99.22%] [G loss: 4.933649]\n",
      "******* 879 [D loss: 0.195664, acc: 92.97%] [G loss: 3.218490]\n",
      "******* 880 [D loss: 0.306756, acc: 85.94%] [G loss: 3.618447]\n",
      "******* 881 [D loss: 0.039504, acc: 99.22%] [G loss: 5.691466]\n",
      "******* 882 [D loss: 0.250302, acc: 92.97%] [G loss: 3.617619]\n",
      "******* 883 [D loss: 0.152363, acc: 96.88%] [G loss: 2.792495]\n",
      "******* 884 [D loss: 0.120608, acc: 98.44%] [G loss: 4.178460]\n",
      "******* 885 [D loss: 0.130303, acc: 95.31%] [G loss: 4.568243]\n",
      "******* 886 [D loss: 0.188422, acc: 96.09%] [G loss: 3.409005]\n",
      "******* 887 [D loss: 0.232595, acc: 91.41%] [G loss: 3.503877]\n",
      "******* 888 [D loss: 0.088795, acc: 98.44%] [G loss: 4.845942]\n",
      "******* 889 [D loss: 0.147159, acc: 94.53%] [G loss: 4.056145]\n",
      "******* 890 [D loss: 0.197554, acc: 93.75%] [G loss: 2.660271]\n",
      "******* 891 [D loss: 0.158089, acc: 96.09%] [G loss: 3.821167]\n",
      "******* 892 [D loss: 0.137253, acc: 96.09%] [G loss: 4.694308]\n",
      "******* 893 [D loss: 0.197111, acc: 92.19%] [G loss: 2.901923]\n",
      "******* 894 [D loss: 0.224590, acc: 89.84%] [G loss: 2.938054]\n",
      "******* 895 [D loss: 0.058290, acc: 100.00%] [G loss: 4.415832]\n",
      "******* 896 [D loss: 0.163135, acc: 95.31%] [G loss: 4.165051]\n",
      "******* 897 [D loss: 0.144007, acc: 98.44%] [G loss: 3.055860]\n",
      "******* 898 [D loss: 0.298107, acc: 90.62%] [G loss: 2.995677]\n",
      "******* 899 [D loss: 0.111196, acc: 96.09%] [G loss: 4.187088]\n",
      "******* 900 [D loss: 0.198009, acc: 94.53%] [G loss: 3.453026]\n",
      "******* 901 [D loss: 0.233053, acc: 91.41%] [G loss: 2.952264]\n",
      "******* 902 [D loss: 0.155773, acc: 95.31%] [G loss: 4.083333]\n",
      "******* 903 [D loss: 0.117555, acc: 96.09%] [G loss: 4.445127]\n",
      "******* 904 [D loss: 0.228432, acc: 95.31%] [G loss: 3.160110]\n",
      "******* 905 [D loss: 0.181952, acc: 93.75%] [G loss: 3.539756]\n",
      "******* 906 [D loss: 0.076861, acc: 97.66%] [G loss: 4.784457]\n",
      "******* 907 [D loss: 0.224939, acc: 92.97%] [G loss: 3.377255]\n",
      "******* 908 [D loss: 0.180787, acc: 92.97%] [G loss: 3.240791]\n",
      "******* 909 [D loss: 0.109913, acc: 96.88%] [G loss: 4.276855]\n",
      "******* 910 [D loss: 0.184489, acc: 92.19%] [G loss: 3.863220]\n",
      "******* 911 [D loss: 0.137533, acc: 96.09%] [G loss: 3.198084]\n",
      "******* 912 [D loss: 0.123313, acc: 96.09%] [G loss: 3.089669]\n",
      "******* 913 [D loss: 0.082814, acc: 99.22%] [G loss: 4.246244]\n",
      "******* 914 [D loss: 0.077012, acc: 98.44%] [G loss: 5.002789]\n",
      "******* 915 [D loss: 0.454997, acc: 83.59%] [G loss: 2.567432]\n",
      "******* 916 [D loss: 0.261055, acc: 92.19%] [G loss: 2.947460]\n",
      "******* 917 [D loss: 0.064659, acc: 97.66%] [G loss: 5.427143]\n",
      "******* 918 [D loss: 0.227824, acc: 92.97%] [G loss: 4.325043]\n",
      "******* 919 [D loss: 0.157715, acc: 96.88%] [G loss: 2.821256]\n",
      "******* 920 [D loss: 0.209654, acc: 92.19%] [G loss: 3.089370]\n",
      "******* 921 [D loss: 0.042979, acc: 100.00%] [G loss: 5.283887]\n",
      "******* 922 [D loss: 0.112910, acc: 96.88%] [G loss: 5.324961]\n",
      "******* 923 [D loss: 0.244916, acc: 89.84%] [G loss: 2.714877]\n",
      "******* 924 [D loss: 0.264795, acc: 88.28%] [G loss: 3.355840]\n",
      "******* 925 [D loss: 0.115929, acc: 97.66%] [G loss: 4.771461]\n",
      "******* 926 [D loss: 0.079735, acc: 99.22%] [G loss: 4.704093]\n",
      "******* 927 [D loss: 0.329026, acc: 89.06%] [G loss: 2.590947]\n",
      "******* 928 [D loss: 0.330627, acc: 85.94%] [G loss: 3.524482]\n",
      "******* 929 [D loss: 0.085732, acc: 99.22%] [G loss: 5.712466]\n",
      "******* 930 [D loss: 0.347255, acc: 81.25%] [G loss: 3.250336]\n",
      "******* 931 [D loss: 0.212277, acc: 91.41%] [G loss: 2.711526]\n",
      "******* 932 [D loss: 0.112081, acc: 96.09%] [G loss: 4.323378]\n",
      "******* 933 [D loss: 0.130980, acc: 93.75%] [G loss: 4.434550]\n",
      "******* 934 [D loss: 0.110752, acc: 95.31%] [G loss: 3.851536]\n",
      "******* 935 [D loss: 0.149772, acc: 96.09%] [G loss: 2.913134]\n",
      "******* 936 [D loss: 0.146746, acc: 95.31%] [G loss: 4.067467]\n",
      "******* 937 [D loss: 0.062528, acc: 99.22%] [G loss: 4.985595]\n",
      "******* 938 [D loss: 0.103226, acc: 97.66%] [G loss: 4.484751]\n",
      "******* 939 [D loss: 0.256416, acc: 90.62%] [G loss: 2.380439]\n",
      "******* 940 [D loss: 0.295844, acc: 84.38%] [G loss: 3.354016]\n",
      "******* 941 [D loss: 0.075873, acc: 98.44%] [G loss: 5.431434]\n",
      "******* 942 [D loss: 0.385738, acc: 85.94%] [G loss: 2.636464]\n",
      "******* 943 [D loss: 0.616564, acc: 68.75%] [G loss: 2.682071]\n",
      "******* 944 [D loss: 0.082690, acc: 98.44%] [G loss: 5.028225]\n",
      "******* 945 [D loss: 0.265744, acc: 91.41%] [G loss: 4.589940]\n",
      "******* 946 [D loss: 0.346399, acc: 87.50%] [G loss: 2.433887]\n",
      "******* 947 [D loss: 0.366600, acc: 80.47%] [G loss: 3.428932]\n",
      "******* 948 [D loss: 0.082242, acc: 97.66%] [G loss: 5.539798]\n",
      "******* 949 [D loss: 0.218307, acc: 92.19%] [G loss: 4.229462]\n",
      "******* 950 [D loss: 0.246294, acc: 93.75%] [G loss: 2.852708]\n",
      "******* 951 [D loss: 0.289085, acc: 82.03%] [G loss: 2.871997]\n",
      "******* 952 [D loss: 0.150435, acc: 95.31%] [G loss: 4.646260]\n",
      "******* 953 [D loss: 0.111157, acc: 95.31%] [G loss: 4.759278]\n",
      "******* 954 [D loss: 0.278881, acc: 90.62%] [G loss: 2.830696]\n",
      "******* 955 [D loss: 0.337579, acc: 82.03%] [G loss: 2.574371]\n",
      "******* 956 [D loss: 0.133849, acc: 97.66%] [G loss: 3.810246]\n",
      "******* 957 [D loss: 0.215621, acc: 91.41%] [G loss: 4.074932]\n",
      "******* 958 [D loss: 0.224680, acc: 93.75%] [G loss: 2.573035]\n",
      "******* 959 [D loss: 0.277273, acc: 91.41%] [G loss: 3.009396]\n",
      "******* 960 [D loss: 0.114351, acc: 97.66%] [G loss: 3.813286]\n",
      "******* 961 [D loss: 0.233715, acc: 91.41%] [G loss: 3.035717]\n",
      "******* 962 [D loss: 0.288160, acc: 89.84%] [G loss: 2.328357]\n",
      "******* 963 [D loss: 0.274969, acc: 88.28%] [G loss: 2.928184]\n",
      "******* 964 [D loss: 0.165282, acc: 95.31%] [G loss: 3.867868]\n",
      "******* 965 [D loss: 0.305962, acc: 90.62%] [G loss: 2.895659]\n",
      "******* 966 [D loss: 0.209863, acc: 95.31%] [G loss: 3.134357]\n",
      "******* 967 [D loss: 0.198910, acc: 92.97%] [G loss: 3.344340]\n",
      "******* 968 [D loss: 0.253923, acc: 92.19%] [G loss: 3.344769]\n",
      "******* 969 [D loss: 0.286370, acc: 91.41%] [G loss: 2.340256]\n",
      "******* 970 [D loss: 0.146509, acc: 98.44%] [G loss: 3.259841]\n",
      "******* 971 [D loss: 0.183784, acc: 95.31%] [G loss: 3.855989]\n",
      "******* 972 [D loss: 0.305448, acc: 89.06%] [G loss: 2.550275]\n",
      "******* 973 [D loss: 0.238685, acc: 92.19%] [G loss: 2.897732]\n",
      "******* 974 [D loss: 0.158031, acc: 95.31%] [G loss: 3.615423]\n",
      "******* 975 [D loss: 0.272572, acc: 89.06%] [G loss: 3.154376]\n",
      "******* 976 [D loss: 0.135633, acc: 96.09%] [G loss: 3.173368]\n",
      "******* 977 [D loss: 0.152396, acc: 96.88%] [G loss: 3.178577]\n",
      "******* 978 [D loss: 0.126982, acc: 95.31%] [G loss: 3.335714]\n",
      "******* 979 [D loss: 0.181687, acc: 93.75%] [G loss: 2.966678]\n",
      "******* 980 [D loss: 0.145825, acc: 96.88%] [G loss: 3.318607]\n",
      "******* 981 [D loss: 0.176347, acc: 96.09%] [G loss: 3.283615]\n",
      "******* 982 [D loss: 0.202734, acc: 96.09%] [G loss: 2.687111]\n",
      "******* 983 [D loss: 0.191450, acc: 96.88%] [G loss: 3.201617]\n",
      "******* 984 [D loss: 0.156317, acc: 95.31%] [G loss: 3.836329]\n",
      "******* 985 [D loss: 0.087782, acc: 97.66%] [G loss: 3.699934]\n",
      "******* 986 [D loss: 0.402010, acc: 89.06%] [G loss: 1.618955]\n",
      "******* 987 [D loss: 0.439349, acc: 75.78%] [G loss: 3.127378]\n",
      "******* 988 [D loss: 0.270754, acc: 92.97%] [G loss: 4.069724]\n",
      "******* 989 [D loss: 0.340262, acc: 89.06%] [G loss: 2.737665]\n",
      "******* 990 [D loss: 0.282325, acc: 90.62%] [G loss: 2.178802]\n",
      "******* 991 [D loss: 0.191738, acc: 96.09%] [G loss: 4.084534]\n",
      "******* 992 [D loss: 0.262700, acc: 92.97%] [G loss: 3.731418]\n",
      "******* 993 [D loss: 0.282150, acc: 91.41%] [G loss: 2.835999]\n",
      "******* 994 [D loss: 0.209310, acc: 93.75%] [G loss: 3.119884]\n",
      "******* 995 [D loss: 0.242706, acc: 92.97%] [G loss: 3.361818]\n",
      "******* 996 [D loss: 0.271477, acc: 92.19%] [G loss: 3.086962]\n",
      "******* 997 [D loss: 0.133278, acc: 96.09%] [G loss: 2.810818]\n",
      "******* 998 [D loss: 0.285762, acc: 92.97%] [G loss: 3.086371]\n",
      "******* 999 [D loss: 0.307620, acc: 85.94%] [G loss: 3.145675]\n",
      "******* 1000 [D loss: 0.292986, acc: 91.41%] [G loss: 2.885705]\n",
      "0.00000006\n",
      "saved\n",
      "******* 1001 [D loss: 0.270051, acc: 90.62%] [G loss: 2.714836]\n",
      "******* 1002 [D loss: 0.154217, acc: 96.09%] [G loss: 3.256956]\n",
      "******* 1003 [D loss: 0.268326, acc: 94.53%] [G loss: 2.981709]\n",
      "******* 1004 [D loss: 0.347852, acc: 84.38%] [G loss: 2.792358]\n",
      "******* 1005 [D loss: 0.303313, acc: 88.28%] [G loss: 2.837904]\n",
      "******* 1006 [D loss: 0.232264, acc: 92.19%] [G loss: 2.786814]\n",
      "******* 1007 [D loss: 0.356216, acc: 88.28%] [G loss: 2.725458]\n",
      "******* 1008 [D loss: 0.315314, acc: 87.50%] [G loss: 2.862996]\n",
      "******* 1009 [D loss: 0.258009, acc: 89.84%] [G loss: 3.261840]\n",
      "******* 1010 [D loss: 0.379097, acc: 89.84%] [G loss: 2.699653]\n",
      "******* 1011 [D loss: 0.563455, acc: 71.09%] [G loss: 2.348966]\n",
      "******* 1012 [D loss: 0.287584, acc: 87.50%] [G loss: 3.709464]\n",
      "******* 1013 [D loss: 0.370959, acc: 88.28%] [G loss: 2.887649]\n",
      "******* 1014 [D loss: 0.372485, acc: 84.38%] [G loss: 1.966009]\n",
      "******* 1015 [D loss: 0.361546, acc: 84.38%] [G loss: 3.301403]\n",
      "******* 1016 [D loss: 0.271537, acc: 89.06%] [G loss: 4.117952]\n",
      "******* 1017 [D loss: 0.370552, acc: 85.16%] [G loss: 2.559331]\n",
      "******* 1018 [D loss: 0.344922, acc: 85.16%] [G loss: 2.344079]\n",
      "******* 1019 [D loss: 0.291796, acc: 90.62%] [G loss: 3.220729]\n",
      "******* 1020 [D loss: 0.522399, acc: 82.03%] [G loss: 2.812665]\n",
      "******* 1021 [D loss: 0.392408, acc: 81.25%] [G loss: 2.760365]\n",
      "******* 1022 [D loss: 0.232478, acc: 92.19%] [G loss: 3.248247]\n",
      "******* 1023 [D loss: 0.504495, acc: 81.25%] [G loss: 2.797656]\n",
      "******* 1024 [D loss: 0.289329, acc: 90.62%] [G loss: 2.720272]\n",
      "******* 1025 [D loss: 0.468935, acc: 79.69%] [G loss: 2.744937]\n",
      "******* 1026 [D loss: 0.380508, acc: 84.38%] [G loss: 2.276448]\n",
      "******* 1027 [D loss: 0.267780, acc: 90.62%] [G loss: 2.837048]\n",
      "******* 1028 [D loss: 0.283188, acc: 92.19%] [G loss: 3.082377]\n",
      "******* 1029 [D loss: 0.502868, acc: 84.38%] [G loss: 2.356544]\n",
      "******* 1030 [D loss: 0.318602, acc: 85.94%] [G loss: 2.751401]\n",
      "******* 1031 [D loss: 0.216619, acc: 92.19%] [G loss: 3.336175]\n",
      "******* 1032 [D loss: 0.233290, acc: 92.19%] [G loss: 3.568260]\n",
      "******* 1033 [D loss: 0.228166, acc: 89.84%] [G loss: 2.686699]\n",
      "******* 1034 [D loss: 0.318038, acc: 85.16%] [G loss: 2.750561]\n",
      "******* 1035 [D loss: 0.222174, acc: 94.53%] [G loss: 3.163166]\n",
      "******* 1036 [D loss: 0.341076, acc: 90.62%] [G loss: 2.796467]\n",
      "******* 1037 [D loss: 0.333427, acc: 86.72%] [G loss: 2.438030]\n",
      "******* 1038 [D loss: 0.267365, acc: 92.97%] [G loss: 2.928097]\n",
      "******* 1039 [D loss: 0.350237, acc: 85.16%] [G loss: 2.747056]\n",
      "******* 1040 [D loss: 0.197378, acc: 92.97%] [G loss: 2.761050]\n",
      "******* 1041 [D loss: 0.167891, acc: 96.88%] [G loss: 3.566472]\n",
      "******* 1042 [D loss: 0.341327, acc: 88.28%] [G loss: 2.804510]\n",
      "******* 1043 [D loss: 0.200145, acc: 96.88%] [G loss: 2.696124]\n",
      "******* 1044 [D loss: 0.207755, acc: 95.31%] [G loss: 3.164115]\n",
      "******* 1045 [D loss: 0.179507, acc: 93.75%] [G loss: 2.830197]\n",
      "******* 1046 [D loss: 0.239594, acc: 91.41%] [G loss: 2.867311]\n",
      "******* 1047 [D loss: 0.286333, acc: 90.62%] [G loss: 2.590250]\n",
      "******* 1048 [D loss: 0.173384, acc: 96.88%] [G loss: 3.284564]\n",
      "******* 1049 [D loss: 0.225982, acc: 92.97%] [G loss: 2.909970]\n",
      "******* 1050 [D loss: 0.417518, acc: 85.16%] [G loss: 1.957583]\n",
      "******* 1051 [D loss: 0.235141, acc: 91.41%] [G loss: 3.262229]\n",
      "******* 1052 [D loss: 0.242506, acc: 92.19%] [G loss: 3.803928]\n",
      "******* 1053 [D loss: 0.418488, acc: 85.16%] [G loss: 2.393807]\n",
      "******* 1054 [D loss: 0.325940, acc: 80.47%] [G loss: 2.627009]\n",
      "******* 1055 [D loss: 0.161358, acc: 96.88%] [G loss: 3.622631]\n",
      "******* 1056 [D loss: 0.333378, acc: 88.28%] [G loss: 2.952615]\n",
      "******* 1057 [D loss: 0.270057, acc: 90.62%] [G loss: 2.502497]\n",
      "******* 1058 [D loss: 0.319394, acc: 85.16%] [G loss: 3.308886]\n",
      "******* 1059 [D loss: 0.273348, acc: 90.62%] [G loss: 2.897551]\n",
      "******* 1060 [D loss: 0.190472, acc: 97.66%] [G loss: 2.896663]\n",
      "******* 1061 [D loss: 0.149690, acc: 94.53%] [G loss: 3.372123]\n",
      "******* 1062 [D loss: 0.302287, acc: 89.84%] [G loss: 2.920749]\n",
      "******* 1063 [D loss: 0.374966, acc: 87.50%] [G loss: 2.457726]\n",
      "******* 1064 [D loss: 0.264674, acc: 89.06%] [G loss: 3.310900]\n",
      "******* 1065 [D loss: 0.362619, acc: 85.94%] [G loss: 2.578205]\n",
      "******* 1066 [D loss: 0.469001, acc: 81.25%] [G loss: 2.262490]\n",
      "******* 1067 [D loss: 0.312756, acc: 88.28%] [G loss: 3.028374]\n",
      "******* 1068 [D loss: 0.360731, acc: 89.06%] [G loss: 2.904299]\n",
      "******* 1069 [D loss: 0.431985, acc: 84.38%] [G loss: 2.284296]\n",
      "******* 1070 [D loss: 0.400190, acc: 82.03%] [G loss: 2.468369]\n",
      "******* 1071 [D loss: 0.249798, acc: 93.75%] [G loss: 3.039167]\n",
      "******* 1072 [D loss: 0.282108, acc: 89.84%] [G loss: 2.660819]\n",
      "******* 1073 [D loss: 0.262373, acc: 91.41%] [G loss: 2.657057]\n",
      "******* 1074 [D loss: 0.195201, acc: 94.53%] [G loss: 3.521016]\n",
      "******* 1075 [D loss: 0.291926, acc: 91.41%] [G loss: 3.331815]\n",
      "******* 1076 [D loss: 0.475155, acc: 81.25%] [G loss: 2.129579]\n",
      "******* 1077 [D loss: 0.332571, acc: 84.38%] [G loss: 2.591593]\n",
      "******* 1078 [D loss: 0.275297, acc: 91.41%] [G loss: 3.301098]\n",
      "******* 1079 [D loss: 0.462136, acc: 84.38%] [G loss: 2.068796]\n",
      "******* 1080 [D loss: 0.471818, acc: 74.22%] [G loss: 2.315193]\n",
      "******* 1081 [D loss: 0.234937, acc: 89.84%] [G loss: 4.229491]\n",
      "******* 1082 [D loss: 0.702080, acc: 73.44%] [G loss: 2.083993]\n",
      "******* 1083 [D loss: 0.400333, acc: 78.12%] [G loss: 2.246280]\n",
      "******* 1084 [D loss: 0.307620, acc: 90.62%] [G loss: 3.373014]\n",
      "******* 1085 [D loss: 0.278444, acc: 89.84%] [G loss: 3.636722]\n",
      "******* 1086 [D loss: 0.468630, acc: 82.03%] [G loss: 2.173604]\n",
      "******* 1087 [D loss: 0.349989, acc: 83.59%] [G loss: 2.557575]\n",
      "******* 1088 [D loss: 0.255263, acc: 92.19%] [G loss: 3.142199]\n",
      "******* 1089 [D loss: 0.285129, acc: 88.28%] [G loss: 2.914464]\n",
      "******* 1090 [D loss: 0.454815, acc: 82.03%] [G loss: 2.108831]\n",
      "******* 1091 [D loss: 0.369257, acc: 86.72%] [G loss: 2.596917]\n",
      "******* 1092 [D loss: 0.246965, acc: 92.19%] [G loss: 2.975934]\n",
      "******* 1093 [D loss: 0.428986, acc: 85.94%] [G loss: 2.781395]\n",
      "******* 1094 [D loss: 0.572532, acc: 74.22%] [G loss: 2.580009]\n",
      "******* 1095 [D loss: 0.295657, acc: 89.06%] [G loss: 3.209100]\n",
      "******* 1096 [D loss: 0.342096, acc: 88.28%] [G loss: 3.075861]\n",
      "******* 1097 [D loss: 0.460740, acc: 79.69%] [G loss: 1.790138]\n",
      "******* 1098 [D loss: 0.325167, acc: 85.16%] [G loss: 2.526843]\n",
      "******* 1099 [D loss: 0.335968, acc: 89.06%] [G loss: 3.255038]\n",
      "******* 1100 [D loss: 0.410231, acc: 87.50%] [G loss: 2.891084]\n",
      "******* 1101 [D loss: 0.579418, acc: 73.44%] [G loss: 1.890110]\n",
      "******* 1102 [D loss: 0.458095, acc: 75.00%] [G loss: 2.692010]\n",
      "******* 1103 [D loss: 0.267416, acc: 92.19%] [G loss: 3.903108]\n",
      "******* 1104 [D loss: 0.577596, acc: 81.25%] [G loss: 2.583341]\n",
      "******* 1105 [D loss: 0.528721, acc: 74.22%] [G loss: 1.772196]\n",
      "******* 1106 [D loss: 0.459412, acc: 78.12%] [G loss: 3.019805]\n",
      "******* 1107 [D loss: 0.398927, acc: 85.94%] [G loss: 3.778761]\n",
      "******* 1108 [D loss: 0.480496, acc: 83.59%] [G loss: 2.489001]\n",
      "******* 1109 [D loss: 0.600117, acc: 67.97%] [G loss: 2.197451]\n",
      "******* 1110 [D loss: 0.221528, acc: 93.75%] [G loss: 3.328584]\n",
      "******* 1111 [D loss: 0.652699, acc: 79.69%] [G loss: 3.018509]\n",
      "******* 1112 [D loss: 0.438959, acc: 80.47%] [G loss: 2.101433]\n",
      "******* 1113 [D loss: 0.416642, acc: 82.81%] [G loss: 2.790469]\n",
      "******* 1114 [D loss: 0.292327, acc: 89.84%] [G loss: 3.210325]\n",
      "******* 1115 [D loss: 0.437016, acc: 80.47%] [G loss: 2.787529]\n",
      "******* 1116 [D loss: 0.354084, acc: 87.50%] [G loss: 2.745763]\n",
      "******* 1117 [D loss: 0.248077, acc: 93.75%] [G loss: 3.330403]\n",
      "******* 1118 [D loss: 0.225946, acc: 95.31%] [G loss: 2.792203]\n",
      "******* 1119 [D loss: 0.459949, acc: 82.03%] [G loss: 2.403702]\n",
      "******* 1120 [D loss: 0.386171, acc: 82.03%] [G loss: 2.900629]\n",
      "******* 1121 [D loss: 0.287945, acc: 89.84%] [G loss: 3.603387]\n",
      "******* 1122 [D loss: 0.529127, acc: 81.25%] [G loss: 2.840647]\n",
      "******* 1123 [D loss: 0.406111, acc: 84.38%] [G loss: 2.485388]\n",
      "******* 1124 [D loss: 0.364528, acc: 82.81%] [G loss: 3.060768]\n",
      "******* 1125 [D loss: 0.296905, acc: 91.41%] [G loss: 3.856497]\n",
      "******* 1126 [D loss: 0.266778, acc: 89.84%] [G loss: 2.973869]\n",
      "******* 1127 [D loss: 0.315627, acc: 86.72%] [G loss: 2.587427]\n",
      "******* 1128 [D loss: 0.325717, acc: 85.94%] [G loss: 3.434765]\n",
      "******* 1129 [D loss: 0.389382, acc: 85.16%] [G loss: 3.146162]\n",
      "******* 1130 [D loss: 0.476743, acc: 83.59%] [G loss: 2.627594]\n",
      "******* 1131 [D loss: 0.307790, acc: 89.06%] [G loss: 2.901125]\n",
      "******* 1132 [D loss: 0.367368, acc: 81.25%] [G loss: 2.550325]\n",
      "******* 1133 [D loss: 0.439010, acc: 85.94%] [G loss: 2.758377]\n",
      "******* 1134 [D loss: 0.343302, acc: 89.06%] [G loss: 2.584380]\n",
      "******* 1135 [D loss: 0.465849, acc: 82.81%] [G loss: 2.441403]\n",
      "******* 1136 [D loss: 0.386461, acc: 85.16%] [G loss: 2.791541]\n",
      "******* 1137 [D loss: 0.251820, acc: 92.19%] [G loss: 3.273012]\n",
      "******* 1138 [D loss: 0.361862, acc: 85.94%] [G loss: 2.847487]\n",
      "******* 1139 [D loss: 0.270085, acc: 90.62%] [G loss: 2.291062]\n",
      "******* 1140 [D loss: 0.230247, acc: 94.53%] [G loss: 2.769165]\n",
      "******* 1141 [D loss: 0.349016, acc: 90.62%] [G loss: 2.778865]\n",
      "******* 1142 [D loss: 0.210672, acc: 94.53%] [G loss: 3.061134]\n",
      "******* 1143 [D loss: 0.421884, acc: 85.94%] [G loss: 2.560255]\n",
      "******* 1144 [D loss: 0.312805, acc: 84.38%] [G loss: 2.567390]\n",
      "******* 1145 [D loss: 0.206930, acc: 93.75%] [G loss: 3.138308]\n",
      "******* 1146 [D loss: 0.324920, acc: 84.38%] [G loss: 2.709651]\n",
      "******* 1147 [D loss: 0.506364, acc: 79.69%] [G loss: 2.138434]\n",
      "******* 1148 [D loss: 0.226086, acc: 91.41%] [G loss: 3.002365]\n",
      "******* 1149 [D loss: 0.275347, acc: 91.41%] [G loss: 3.468013]\n",
      "******* 1150 [D loss: 0.264879, acc: 92.19%] [G loss: 2.815978]\n",
      "******* 1151 [D loss: 0.307998, acc: 92.19%] [G loss: 2.478785]\n",
      "******* 1152 [D loss: 0.269912, acc: 90.62%] [G loss: 2.944180]\n",
      "******* 1153 [D loss: 0.253378, acc: 92.19%] [G loss: 2.726136]\n",
      "******* 1154 [D loss: 0.386044, acc: 85.16%] [G loss: 2.360559]\n",
      "******* 1155 [D loss: 0.353055, acc: 90.62%] [G loss: 2.574778]\n",
      "******* 1156 [D loss: 0.291239, acc: 89.06%] [G loss: 3.019669]\n",
      "******* 1157 [D loss: 0.381457, acc: 81.25%] [G loss: 2.712175]\n",
      "******* 1158 [D loss: 0.310469, acc: 89.06%] [G loss: 3.013654]\n",
      "******* 1159 [D loss: 0.399794, acc: 87.50%] [G loss: 2.749975]\n",
      "******* 1160 [D loss: 0.314996, acc: 86.72%] [G loss: 2.681215]\n",
      "******* 1161 [D loss: 0.361572, acc: 88.28%] [G loss: 2.272851]\n",
      "******* 1162 [D loss: 0.556500, acc: 78.12%] [G loss: 2.582562]\n",
      "******* 1163 [D loss: 0.293982, acc: 92.19%] [G loss: 2.967250]\n",
      "******* 1164 [D loss: 0.464303, acc: 81.25%] [G loss: 2.601278]\n",
      "******* 1165 [D loss: 0.457978, acc: 81.25%] [G loss: 2.362694]\n",
      "******* 1166 [D loss: 0.305636, acc: 83.59%] [G loss: 2.920897]\n",
      "******* 1167 [D loss: 0.400241, acc: 85.94%] [G loss: 2.500752]\n",
      "******* 1168 [D loss: 0.319467, acc: 83.59%] [G loss: 2.234242]\n",
      "******* 1169 [D loss: 0.311293, acc: 89.06%] [G loss: 2.552657]\n",
      "******* 1170 [D loss: 0.358424, acc: 89.84%] [G loss: 2.646030]\n",
      "******* 1171 [D loss: 0.373724, acc: 85.94%] [G loss: 2.726471]\n",
      "******* 1172 [D loss: 0.343103, acc: 88.28%] [G loss: 2.584078]\n",
      "******* 1173 [D loss: 0.319258, acc: 88.28%] [G loss: 2.507959]\n",
      "******* 1174 [D loss: 0.454491, acc: 85.94%] [G loss: 2.219920]\n",
      "******* 1175 [D loss: 0.395399, acc: 84.38%] [G loss: 2.464443]\n",
      "******* 1176 [D loss: 0.413600, acc: 85.16%] [G loss: 2.752262]\n",
      "******* 1177 [D loss: 0.407737, acc: 84.38%] [G loss: 2.272731]\n",
      "******* 1178 [D loss: 0.302008, acc: 87.50%] [G loss: 2.440591]\n",
      "******* 1179 [D loss: 0.369740, acc: 90.62%] [G loss: 2.593654]\n",
      "******* 1180 [D loss: 0.411164, acc: 84.38%] [G loss: 2.684399]\n",
      "******* 1181 [D loss: 0.414793, acc: 85.16%] [G loss: 2.757106]\n",
      "******* 1182 [D loss: 0.398349, acc: 86.72%] [G loss: 2.622918]\n",
      "******* 1183 [D loss: 0.380247, acc: 88.28%] [G loss: 2.110219]\n",
      "******* 1184 [D loss: 0.360348, acc: 85.16%] [G loss: 2.318047]\n",
      "******* 1185 [D loss: 0.223915, acc: 92.97%] [G loss: 3.397195]\n",
      "******* 1186 [D loss: 0.515362, acc: 81.25%] [G loss: 2.564722]\n",
      "******* 1187 [D loss: 0.259514, acc: 89.84%] [G loss: 2.629064]\n",
      "******* 1188 [D loss: 0.288159, acc: 92.19%] [G loss: 3.304731]\n",
      "******* 1189 [D loss: 0.431987, acc: 82.03%] [G loss: 2.611046]\n",
      "******* 1190 [D loss: 0.350270, acc: 82.81%] [G loss: 2.411978]\n",
      "******* 1191 [D loss: 0.267961, acc: 91.41%] [G loss: 3.054814]\n",
      "******* 1192 [D loss: 0.272265, acc: 95.31%] [G loss: 3.046073]\n",
      "******* 1193 [D loss: 0.208181, acc: 92.97%] [G loss: 2.934546]\n",
      "******* 1194 [D loss: 0.231018, acc: 90.62%] [G loss: 2.556811]\n",
      "******* 1195 [D loss: 0.398998, acc: 85.16%] [G loss: 2.112062]\n",
      "******* 1196 [D loss: 0.258386, acc: 90.62%] [G loss: 2.562923]\n",
      "******* 1197 [D loss: 0.423567, acc: 81.25%] [G loss: 2.350246]\n",
      "******* 1198 [D loss: 0.245202, acc: 92.19%] [G loss: 2.906531]\n",
      "******* 1199 [D loss: 0.286600, acc: 89.84%] [G loss: 2.823332]\n",
      "******* 1200 [D loss: 0.414852, acc: 86.72%] [G loss: 2.235751]\n",
      "0.00000007\n",
      "saved\n",
      "******* 1201 [D loss: 0.326516, acc: 86.72%] [G loss: 2.312655]\n",
      "******* 1202 [D loss: 0.387506, acc: 86.72%] [G loss: 2.450532]\n",
      "******* 1203 [D loss: 0.296927, acc: 90.62%] [G loss: 3.444578]\n",
      "******* 1204 [D loss: 0.411561, acc: 85.16%] [G loss: 2.799857]\n",
      "******* 1205 [D loss: 0.414478, acc: 81.25%] [G loss: 2.268044]\n",
      "******* 1206 [D loss: 0.287473, acc: 86.72%] [G loss: 2.632160]\n",
      "******* 1207 [D loss: 0.331223, acc: 87.50%] [G loss: 3.141495]\n",
      "******* 1208 [D loss: 0.416202, acc: 81.25%] [G loss: 2.948084]\n",
      "******* 1209 [D loss: 0.338769, acc: 89.06%] [G loss: 2.385967]\n",
      "******* 1210 [D loss: 0.297155, acc: 89.84%] [G loss: 2.472960]\n",
      "******* 1211 [D loss: 0.425601, acc: 82.03%] [G loss: 2.451520]\n",
      "******* 1212 [D loss: 0.284852, acc: 90.62%] [G loss: 2.765576]\n",
      "******* 1213 [D loss: 0.355850, acc: 85.16%] [G loss: 2.459344]\n",
      "******* 1214 [D loss: 0.307353, acc: 90.62%] [G loss: 2.281143]\n",
      "******* 1215 [D loss: 0.395408, acc: 81.25%] [G loss: 2.234903]\n",
      "******* 1216 [D loss: 0.277290, acc: 88.28%] [G loss: 2.782540]\n",
      "******* 1217 [D loss: 0.338225, acc: 85.16%] [G loss: 2.931989]\n",
      "******* 1218 [D loss: 0.434332, acc: 84.38%] [G loss: 2.477315]\n",
      "******* 1219 [D loss: 0.504253, acc: 77.34%] [G loss: 2.344491]\n",
      "******* 1220 [D loss: 0.201133, acc: 92.97%] [G loss: 3.373034]\n",
      "******* 1221 [D loss: 0.487102, acc: 79.69%] [G loss: 3.172612]\n",
      "******* 1222 [D loss: 0.313042, acc: 87.50%] [G loss: 2.534668]\n",
      "******* 1223 [D loss: 0.359709, acc: 87.50%] [G loss: 2.266484]\n",
      "******* 1224 [D loss: 0.279629, acc: 89.84%] [G loss: 2.685425]\n",
      "******* 1225 [D loss: 0.233518, acc: 92.97%] [G loss: 3.207709]\n",
      "******* 1226 [D loss: 0.435472, acc: 86.72%] [G loss: 2.488094]\n",
      "******* 1227 [D loss: 0.325976, acc: 88.28%] [G loss: 2.549890]\n",
      "******* 1228 [D loss: 0.215978, acc: 93.75%] [G loss: 3.101293]\n",
      "******* 1229 [D loss: 0.366115, acc: 88.28%] [G loss: 2.641769]\n",
      "******* 1230 [D loss: 0.391058, acc: 85.94%] [G loss: 2.043364]\n",
      "******* 1231 [D loss: 0.401261, acc: 83.59%] [G loss: 2.676761]\n",
      "******* 1232 [D loss: 0.340524, acc: 86.72%] [G loss: 2.937479]\n",
      "******* 1233 [D loss: 0.570996, acc: 78.12%] [G loss: 2.288325]\n",
      "******* 1234 [D loss: 0.548761, acc: 75.00%] [G loss: 1.998846]\n",
      "******* 1235 [D loss: 0.379547, acc: 84.38%] [G loss: 2.828496]\n",
      "******* 1236 [D loss: 0.373652, acc: 85.16%] [G loss: 3.048118]\n",
      "******* 1237 [D loss: 0.330959, acc: 85.94%] [G loss: 2.699762]\n",
      "******* 1238 [D loss: 0.485036, acc: 79.69%] [G loss: 2.186764]\n",
      "******* 1239 [D loss: 0.313626, acc: 87.50%] [G loss: 2.580767]\n",
      "******* 1240 [D loss: 0.278638, acc: 90.62%] [G loss: 3.363744]\n",
      "******* 1241 [D loss: 0.386410, acc: 88.28%] [G loss: 2.842773]\n",
      "******* 1242 [D loss: 0.411051, acc: 78.12%] [G loss: 2.173757]\n",
      "******* 1243 [D loss: 0.408828, acc: 82.81%] [G loss: 2.811896]\n",
      "******* 1244 [D loss: 0.279117, acc: 90.62%] [G loss: 3.234170]\n",
      "******* 1245 [D loss: 0.450651, acc: 86.72%] [G loss: 2.523404]\n",
      "******* 1246 [D loss: 0.312357, acc: 90.62%] [G loss: 2.304185]\n",
      "******* 1247 [D loss: 0.273460, acc: 89.84%] [G loss: 3.077589]\n",
      "******* 1248 [D loss: 0.280799, acc: 89.84%] [G loss: 3.399413]\n",
      "******* 1249 [D loss: 0.448273, acc: 81.25%] [G loss: 2.383290]\n",
      "******* 1250 [D loss: 0.338873, acc: 83.59%] [G loss: 1.955723]\n",
      "******* 1251 [D loss: 0.249091, acc: 89.06%] [G loss: 2.903325]\n",
      "******* 1252 [D loss: 0.409063, acc: 84.38%] [G loss: 2.910357]\n",
      "******* 1253 [D loss: 0.335043, acc: 90.62%] [G loss: 2.735055]\n",
      "******* 1254 [D loss: 0.329086, acc: 86.72%] [G loss: 2.514994]\n",
      "******* 1255 [D loss: 0.330140, acc: 89.06%] [G loss: 2.412298]\n",
      "******* 1256 [D loss: 0.551962, acc: 78.91%] [G loss: 2.221435]\n",
      "******* 1257 [D loss: 0.431825, acc: 85.16%] [G loss: 2.644880]\n",
      "******* 1258 [D loss: 0.417739, acc: 85.94%] [G loss: 2.403712]\n",
      "******* 1259 [D loss: 0.628469, acc: 72.66%] [G loss: 2.240457]\n",
      "******* 1260 [D loss: 0.271079, acc: 89.06%] [G loss: 2.753064]\n",
      "******* 1261 [D loss: 0.443073, acc: 83.59%] [G loss: 2.829561]\n",
      "******* 1262 [D loss: 0.375653, acc: 86.72%] [G loss: 2.627042]\n",
      "******* 1263 [D loss: 0.298763, acc: 88.28%] [G loss: 2.607594]\n",
      "******* 1264 [D loss: 0.650407, acc: 76.56%] [G loss: 2.400367]\n",
      "******* 1265 [D loss: 0.295138, acc: 89.06%] [G loss: 2.600819]\n",
      "******* 1266 [D loss: 0.413170, acc: 78.91%] [G loss: 2.850271]\n",
      "******* 1267 [D loss: 0.417185, acc: 82.03%] [G loss: 2.586713]\n",
      "******* 1268 [D loss: 0.443146, acc: 77.34%] [G loss: 2.731063]\n",
      "******* 1269 [D loss: 0.266660, acc: 90.62%] [G loss: 2.837025]\n",
      "******* 1270 [D loss: 0.291660, acc: 89.06%] [G loss: 2.814643]\n",
      "******* 1271 [D loss: 0.326826, acc: 89.84%] [G loss: 2.661965]\n",
      "******* 1272 [D loss: 0.296203, acc: 88.28%] [G loss: 2.632842]\n",
      "******* 1273 [D loss: 0.298066, acc: 87.50%] [G loss: 2.737261]\n",
      "******* 1274 [D loss: 0.273270, acc: 91.41%] [G loss: 2.724398]\n",
      "******* 1275 [D loss: 0.286270, acc: 89.84%] [G loss: 2.745327]\n",
      "******* 1276 [D loss: 0.296763, acc: 86.72%] [G loss: 2.762965]\n",
      "******* 1277 [D loss: 0.326540, acc: 88.28%] [G loss: 2.606431]\n",
      "******* 1278 [D loss: 0.396868, acc: 82.03%] [G loss: 2.527878]\n",
      "******* 1279 [D loss: 0.387660, acc: 85.94%] [G loss: 2.646309]\n",
      "******* 1280 [D loss: 0.337914, acc: 90.62%] [G loss: 2.563419]\n",
      "******* 1281 [D loss: 0.316376, acc: 89.06%] [G loss: 2.814539]\n",
      "******* 1282 [D loss: 0.368202, acc: 88.28%] [G loss: 2.588596]\n",
      "******* 1283 [D loss: 0.430070, acc: 82.81%] [G loss: 2.343332]\n",
      "******* 1284 [D loss: 0.375520, acc: 86.72%] [G loss: 2.558701]\n",
      "******* 1285 [D loss: 0.286581, acc: 90.62%] [G loss: 2.646339]\n",
      "******* 1286 [D loss: 0.265900, acc: 87.50%] [G loss: 2.921606]\n",
      "******* 1287 [D loss: 0.360589, acc: 89.06%] [G loss: 2.443977]\n",
      "******* 1288 [D loss: 0.235540, acc: 92.97%] [G loss: 2.491027]\n",
      "******* 1289 [D loss: 0.185327, acc: 96.09%] [G loss: 2.861592]\n",
      "******* 1290 [D loss: 0.302369, acc: 90.62%] [G loss: 2.511476]\n",
      "******* 1291 [D loss: 0.292485, acc: 90.62%] [G loss: 2.108175]\n",
      "******* 1292 [D loss: 0.301330, acc: 88.28%] [G loss: 2.431812]\n",
      "******* 1293 [D loss: 0.242056, acc: 91.41%] [G loss: 3.404098]\n",
      "******* 1294 [D loss: 0.306740, acc: 90.62%] [G loss: 2.963563]\n",
      "******* 1295 [D loss: 0.439137, acc: 82.81%] [G loss: 2.159461]\n",
      "******* 1296 [D loss: 0.414762, acc: 83.59%] [G loss: 2.387147]\n",
      "******* 1297 [D loss: 0.278246, acc: 89.84%] [G loss: 3.180464]\n",
      "******* 1298 [D loss: 0.301121, acc: 90.62%] [G loss: 3.213466]\n",
      "******* 1299 [D loss: 0.266526, acc: 89.06%] [G loss: 2.967264]\n",
      "******* 1300 [D loss: 0.231591, acc: 92.19%] [G loss: 2.429892]\n",
      "******* 1301 [D loss: 0.222416, acc: 93.75%] [G loss: 2.591597]\n",
      "******* 1302 [D loss: 0.213455, acc: 93.75%] [G loss: 2.709398]\n",
      "******* 1303 [D loss: 0.147372, acc: 96.88%] [G loss: 3.099412]\n",
      "******* 1304 [D loss: 0.392540, acc: 85.16%] [G loss: 2.515847]\n",
      "******* 1305 [D loss: 0.227130, acc: 94.53%] [G loss: 2.813668]\n",
      "******* 1306 [D loss: 0.161400, acc: 95.31%] [G loss: 2.828050]\n",
      "******* 1307 [D loss: 0.265182, acc: 89.06%] [G loss: 2.483256]\n",
      "******* 1308 [D loss: 0.263319, acc: 89.84%] [G loss: 2.549897]\n",
      "******* 1309 [D loss: 0.234379, acc: 92.19%] [G loss: 3.043936]\n",
      "******* 1310 [D loss: 0.290231, acc: 90.62%] [G loss: 3.073622]\n",
      "******* 1311 [D loss: 0.282592, acc: 92.19%] [G loss: 2.688393]\n",
      "******* 1312 [D loss: 0.323628, acc: 87.50%] [G loss: 2.549868]\n",
      "******* 1313 [D loss: 0.201990, acc: 96.09%] [G loss: 2.961147]\n",
      "******* 1314 [D loss: 0.484452, acc: 79.69%] [G loss: 2.348148]\n",
      "******* 1315 [D loss: 0.311308, acc: 87.50%] [G loss: 2.516011]\n",
      "******* 1316 [D loss: 0.181135, acc: 94.53%] [G loss: 3.270573]\n",
      "******* 1317 [D loss: 0.315611, acc: 91.41%] [G loss: 2.618937]\n",
      "******* 1318 [D loss: 0.461424, acc: 81.25%] [G loss: 2.384540]\n",
      "******* 1319 [D loss: 0.266458, acc: 91.41%] [G loss: 2.819365]\n",
      "******* 1320 [D loss: 0.207204, acc: 90.62%] [G loss: 3.169270]\n",
      "******* 1321 [D loss: 0.424746, acc: 85.16%] [G loss: 2.461073]\n",
      "******* 1322 [D loss: 0.310905, acc: 90.62%] [G loss: 2.303052]\n",
      "******* 1323 [D loss: 0.416332, acc: 85.94%] [G loss: 2.558017]\n",
      "******* 1324 [D loss: 0.373071, acc: 83.59%] [G loss: 2.442533]\n",
      "******* 1325 [D loss: 0.408448, acc: 81.25%] [G loss: 2.518936]\n",
      "******* 1326 [D loss: 0.311785, acc: 88.28%] [G loss: 2.699872]\n",
      "******* 1327 [D loss: 0.246233, acc: 94.53%] [G loss: 2.909795]\n",
      "******* 1328 [D loss: 0.352568, acc: 85.94%] [G loss: 2.866242]\n",
      "******* 1329 [D loss: 0.403911, acc: 81.25%] [G loss: 2.434125]\n",
      "******* 1330 [D loss: 0.356353, acc: 88.28%] [G loss: 2.552811]\n",
      "******* 1331 [D loss: 0.259450, acc: 90.62%] [G loss: 2.699530]\n",
      "******* 1332 [D loss: 0.187130, acc: 96.09%] [G loss: 2.947080]\n",
      "******* 1333 [D loss: 0.276069, acc: 90.62%] [G loss: 2.668035]\n",
      "******* 1334 [D loss: 0.317915, acc: 85.16%] [G loss: 2.587199]\n",
      "******* 1335 [D loss: 0.270825, acc: 90.62%] [G loss: 2.681940]\n",
      "******* 1336 [D loss: 0.581727, acc: 77.34%] [G loss: 2.280219]\n",
      "******* 1337 [D loss: 0.318099, acc: 88.28%] [G loss: 2.538811]\n",
      "******* 1338 [D loss: 0.362900, acc: 85.94%] [G loss: 2.571321]\n",
      "******* 1339 [D loss: 0.289183, acc: 91.41%] [G loss: 2.780364]\n",
      "******* 1340 [D loss: 0.291935, acc: 88.28%] [G loss: 2.674257]\n",
      "******* 1341 [D loss: 0.330071, acc: 87.50%] [G loss: 2.396394]\n",
      "******* 1342 [D loss: 0.358696, acc: 86.72%] [G loss: 2.379003]\n",
      "******* 1343 [D loss: 0.429108, acc: 86.72%] [G loss: 2.436389]\n",
      "******* 1344 [D loss: 0.396725, acc: 85.16%] [G loss: 2.433176]\n",
      "******* 1345 [D loss: 0.284002, acc: 91.41%] [G loss: 2.928085]\n",
      "******* 1346 [D loss: 0.262824, acc: 92.19%] [G loss: 3.160957]\n",
      "******* 1347 [D loss: 0.222834, acc: 93.75%] [G loss: 2.828853]\n",
      "******* 1348 [D loss: 0.372601, acc: 83.59%] [G loss: 2.264264]\n",
      "******* 1349 [D loss: 0.267167, acc: 88.28%] [G loss: 2.431053]\n",
      "******* 1350 [D loss: 0.341184, acc: 87.50%] [G loss: 2.767694]\n",
      "******* 1351 [D loss: 0.404145, acc: 89.06%] [G loss: 2.670732]\n",
      "******* 1352 [D loss: 0.307889, acc: 83.59%] [G loss: 2.351129]\n",
      "******* 1353 [D loss: 0.411486, acc: 81.25%] [G loss: 2.466600]\n",
      "******* 1354 [D loss: 0.296582, acc: 89.06%] [G loss: 2.701816]\n",
      "******* 1355 [D loss: 0.231348, acc: 94.53%] [G loss: 2.997295]\n",
      "******* 1356 [D loss: 0.310773, acc: 87.50%] [G loss: 2.421203]\n",
      "******* 1357 [D loss: 0.507862, acc: 78.12%] [G loss: 2.124779]\n",
      "******* 1358 [D loss: 0.376700, acc: 85.16%] [G loss: 2.832334]\n",
      "******* 1359 [D loss: 0.251834, acc: 89.84%] [G loss: 3.043344]\n",
      "******* 1360 [D loss: 0.403343, acc: 85.16%] [G loss: 2.524268]\n",
      "******* 1361 [D loss: 0.316039, acc: 90.62%] [G loss: 2.355466]\n",
      "******* 1362 [D loss: 0.263565, acc: 92.97%] [G loss: 3.158279]\n",
      "******* 1363 [D loss: 0.301166, acc: 90.62%] [G loss: 3.260465]\n",
      "******* 1364 [D loss: 0.263855, acc: 92.97%] [G loss: 2.844676]\n",
      "******* 1365 [D loss: 0.252963, acc: 88.28%] [G loss: 2.202398]\n",
      "******* 1366 [D loss: 0.285211, acc: 92.19%] [G loss: 2.222954]\n",
      "******* 1367 [D loss: 0.226527, acc: 92.19%] [G loss: 3.107103]\n",
      "******* 1368 [D loss: 0.287891, acc: 88.28%] [G loss: 2.919259]\n",
      "******* 1369 [D loss: 0.297908, acc: 89.84%] [G loss: 2.568102]\n",
      "******* 1370 [D loss: 0.211086, acc: 94.53%] [G loss: 2.734793]\n",
      "******* 1371 [D loss: 0.195056, acc: 92.97%] [G loss: 2.757786]\n",
      "******* 1372 [D loss: 0.309790, acc: 90.62%] [G loss: 2.364678]\n",
      "******* 1373 [D loss: 0.316513, acc: 89.84%] [G loss: 2.620594]\n",
      "******* 1374 [D loss: 0.210204, acc: 92.19%] [G loss: 3.000169]\n",
      "******* 1375 [D loss: 0.250824, acc: 90.62%] [G loss: 2.677005]\n",
      "******* 1376 [D loss: 0.251171, acc: 92.97%] [G loss: 2.896012]\n",
      "******* 1377 [D loss: 0.328700, acc: 92.19%] [G loss: 2.497256]\n",
      "******* 1378 [D loss: 0.233599, acc: 93.75%] [G loss: 2.524284]\n",
      "******* 1379 [D loss: 0.397288, acc: 89.84%] [G loss: 2.515063]\n",
      "******* 1380 [D loss: 0.292989, acc: 88.28%] [G loss: 2.412499]\n",
      "******* 1381 [D loss: 0.210505, acc: 93.75%] [G loss: 2.912497]\n",
      "******* 1382 [D loss: 0.196621, acc: 91.41%] [G loss: 2.893643]\n",
      "******* 1383 [D loss: 0.245782, acc: 91.41%] [G loss: 2.706867]\n",
      "******* 1384 [D loss: 0.283210, acc: 88.28%] [G loss: 2.499183]\n",
      "******* 1385 [D loss: 0.353592, acc: 86.72%] [G loss: 2.724551]\n",
      "******* 1386 [D loss: 0.420332, acc: 85.94%] [G loss: 2.453873]\n",
      "******* 1387 [D loss: 0.466850, acc: 82.03%] [G loss: 2.022915]\n",
      "******* 1388 [D loss: 0.498669, acc: 80.47%] [G loss: 2.205318]\n",
      "******* 1389 [D loss: 0.171838, acc: 96.09%] [G loss: 3.603861]\n",
      "******* 1390 [D loss: 0.257657, acc: 90.62%] [G loss: 2.975836]\n",
      "******* 1391 [D loss: 0.373396, acc: 85.94%] [G loss: 1.958883]\n",
      "******* 1392 [D loss: 0.325246, acc: 85.16%] [G loss: 2.319491]\n",
      "******* 1393 [D loss: 0.320257, acc: 90.62%] [G loss: 3.060939]\n",
      "******* 1394 [D loss: 0.419307, acc: 82.81%] [G loss: 2.691932]\n",
      "******* 1395 [D loss: 0.271916, acc: 89.84%] [G loss: 2.507953]\n",
      "******* 1396 [D loss: 0.228607, acc: 95.31%] [G loss: 2.557543]\n",
      "******* 1397 [D loss: 0.252098, acc: 93.75%] [G loss: 2.985239]\n",
      "******* 1398 [D loss: 0.354600, acc: 85.94%] [G loss: 2.538942]\n",
      "******* 1399 [D loss: 0.456044, acc: 79.69%] [G loss: 1.808113]\n",
      "******* 1400 [D loss: 0.341287, acc: 85.94%] [G loss: 2.507977]\n",
      "0.00000008\n",
      "saved\n",
      "******* 1401 [D loss: 0.316340, acc: 92.97%] [G loss: 2.980464]\n",
      "******* 1402 [D loss: 0.291324, acc: 92.97%] [G loss: 3.020869]\n",
      "******* 1403 [D loss: 0.450870, acc: 85.16%] [G loss: 2.001269]\n",
      "******* 1404 [D loss: 0.447140, acc: 80.47%] [G loss: 1.837576]\n",
      "******* 1405 [D loss: 0.323666, acc: 86.72%] [G loss: 2.787233]\n",
      "******* 1406 [D loss: 0.303605, acc: 91.41%] [G loss: 2.872277]\n",
      "******* 1407 [D loss: 0.449671, acc: 80.47%] [G loss: 2.007317]\n",
      "******* 1408 [D loss: 0.508198, acc: 75.00%] [G loss: 2.142634]\n",
      "******* 1409 [D loss: 0.461165, acc: 79.69%] [G loss: 2.527380]\n",
      "******* 1410 [D loss: 0.336793, acc: 89.06%] [G loss: 2.740040]\n",
      "******* 1411 [D loss: 0.462062, acc: 79.69%] [G loss: 2.039150]\n",
      "******* 1412 [D loss: 0.546735, acc: 74.22%] [G loss: 2.174914]\n",
      "******* 1413 [D loss: 0.444762, acc: 84.38%] [G loss: 2.641024]\n",
      "******* 1414 [D loss: 0.456630, acc: 82.81%] [G loss: 2.280263]\n",
      "******* 1415 [D loss: 0.808207, acc: 60.94%] [G loss: 1.651874]\n",
      "******* 1416 [D loss: 0.473933, acc: 79.69%] [G loss: 2.054279]\n",
      "******* 1417 [D loss: 0.426258, acc: 85.94%] [G loss: 2.530719]\n",
      "******* 1418 [D loss: 0.520580, acc: 80.47%] [G loss: 2.342752]\n",
      "******* 1419 [D loss: 0.472847, acc: 78.91%] [G loss: 2.177839]\n",
      "******* 1420 [D loss: 0.467689, acc: 79.69%] [G loss: 2.130410]\n",
      "******* 1421 [D loss: 0.577732, acc: 75.00%] [G loss: 2.041739]\n",
      "******* 1422 [D loss: 0.566150, acc: 75.00%] [G loss: 2.269859]\n",
      "******* 1423 [D loss: 0.448033, acc: 82.81%] [G loss: 2.302845]\n",
      "******* 1424 [D loss: 0.459877, acc: 83.59%] [G loss: 2.283903]\n",
      "******* 1425 [D loss: 0.338361, acc: 85.16%] [G loss: 2.612812]\n",
      "******* 1426 [D loss: 0.303838, acc: 91.41%] [G loss: 2.892674]\n",
      "******* 1427 [D loss: 0.437156, acc: 83.59%] [G loss: 2.254813]\n",
      "******* 1428 [D loss: 0.515437, acc: 76.56%] [G loss: 2.030696]\n",
      "******* 1429 [D loss: 0.415095, acc: 80.47%] [G loss: 2.511425]\n",
      "******* 1430 [D loss: 0.545762, acc: 80.47%] [G loss: 2.325009]\n",
      "******* 1431 [D loss: 0.425779, acc: 85.94%] [G loss: 2.478931]\n",
      "******* 1432 [D loss: 0.638433, acc: 71.88%] [G loss: 1.904273]\n",
      "******* 1433 [D loss: 0.517300, acc: 72.66%] [G loss: 1.869760]\n"
     ]
    }
   ],
   "source": [
    "def train(epochs, batch_size=64, save_interval=200):\n",
    "  (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "  # print(X_train.shape)\n",
    "  #Rescale data between -1 and 1\n",
    "  X_train = X_train / 127.5 -1.\n",
    "  # X_train = np.expand_dims(X_train, axis=3)\n",
    "  # print(X_train.shape)\n",
    "\n",
    "  #Create our Y for our Neural Networks\n",
    "  valid = np.ones((batch_size, 1))\n",
    "  fakes = np.zeros((batch_size, 1))\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    #Get Random Batch\n",
    "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "    imgs = X_train[idx]\n",
    "\n",
    "    #Generate Fake Images\n",
    "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "\n",
    "    #Train discriminator\n",
    "    d_loss_real = discriminator.train_on_batch(imgs, valid)\n",
    "    d_loss_fake = discriminator.train_on_batch(gen_imgs, fakes)\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "    \n",
    "    #inverse y label\n",
    "    g_loss = GAN.train_on_batch(noise, valid)\n",
    "\n",
    "    print(\"******* %d [D loss: %f, acc: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100* d_loss[1], g_loss))\n",
    "\n",
    "    if(epoch % save_interval) == 0:\n",
    "      save_imgs(epoch)\n",
    "\n",
    "  # print(valid)\n",
    "\n",
    "\n",
    "train(30000, batch_size=64, save_interval=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "po-jSQoN1Azl"
   },
   "source": [
    "### **8) Making GIF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "id": "XPShgQpg1EMy",
    "outputId": "2338a3d3-e58d-4b9d-e789-20d410ad3dd1"
   },
   "outputs": [],
   "source": [
    "# Display a single image using the epoch number\n",
    "# def display_image(epoch_no):\n",
    "#   return PIL.Image.open('generated_images/%.8f.png'.format(epoch_no))\n",
    "\n",
    "anim_file = 'dcgan.gif'\n",
    "\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "  filenames = glob.glob('generated_images/*.png')\n",
    "  filenames = sorted(filenames)\n",
    "  for filename in filenames:\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "  image = imageio.imread(filename)\n",
    "  writer.append_data(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wh37uv1torG5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "2WaNhBDwRwTG"
   ],
   "name": "NumberGAN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
