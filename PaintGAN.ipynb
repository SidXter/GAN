{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ColorGAN_Solution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVnF24NFcuQ6"
      },
      "source": [
        "## **Mount Google Drive to store files & data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzVT_cB7cZFp",
        "outputId": "bac3a926-ef26-4d6e-f5f8-c4dea0f96f57"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJp-D51g0IDd"
      },
      "source": [
        "## **1) Importing Python Packages for GAN**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1k5mFBuzzl2a"
      },
      "source": [
        "# from keras.datasets import cifar10, mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Conv2D, Dense, Conv2DTranspose\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "!mkdir generated_images resized_images"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoRqPt1DwtD_"
      },
      "source": [
        "##!unzip the-zip-file -d name-of-destination-folder\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRWx5D09dX3p"
      },
      "source": [
        "images_path = \"/content/drive/MyDrive/bobross2/\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaoY5WTbi4dJ"
      },
      "source": [
        "## **Resizing Data to match Neural Network Input**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUJtmq0gzkSx"
      },
      "source": [
        "\n",
        "import os\n",
        "# from PIL import Image\n",
        "import cv2\n",
        "reshape_size = (64,64)\n",
        "\n",
        "i = 0\n",
        "for image in os.listdir(images_path):\n",
        "  # print(image)\n",
        "  img = cv2.imread(images_path + image)\n",
        "  img = cv2.resize(img, reshape_size)\n",
        "  cv2.imwrite(\"resized_images/%d.png\" % i,img)\n",
        "  # # print(img.shape)\n",
        "  i = i+1\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr-eZOzg0X79"
      },
      "source": [
        "## **2) Parameters for Neural Networks & Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RThZMDruz9cB",
        "outputId": "7ea397f9-0b81-4253-a300-794478e5745e"
      },
      "source": [
        "img_width = 64\n",
        "img_height = 64\n",
        "channels = 3\n",
        "img_shape = (img_width, img_height, channels)\n",
        "latent_dim = 100\n",
        "adam = Adam(lr=0.0002)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3bcJZZg0cqy"
      },
      "source": [
        "## **3) Building Generator**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdiqZpri0iQh",
        "outputId": "6e2dd4b8-0d8f-4484-e3f0-07fd1406db5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def build_generator():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256 * 8* 8, input_dim=latent_dim))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Reshape((8,8,256)))\n",
        "\n",
        "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Conv2D(3, (3,3), activation='tanh', padding='same'))\n",
        "  \n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "generator = build_generator()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 16384)             1654784   \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 16384)             0         \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 8, 8, 256)         0         \n",
            "                                                                 \n",
            " conv2d_transpose (Conv2DTra  (None, 16, 16, 128)      524416    \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2DT  (None, 32, 32, 128)      262272    \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 32, 32, 128)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_2 (Conv2DT  (None, 64, 64, 128)      262272    \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 64, 64, 128)       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 64, 64, 3)         3459      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,707,203\n",
            "Trainable params: 2,707,203\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt6QsJCW0mcI"
      },
      "source": [
        "## **4) Building Discriminator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2JzEAPv0lKt",
        "outputId": "99a62074-9925-430e-e6a0-950d77039698",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def build_discriminator():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(64, (3,3), padding='same', input_shape=img_shape))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Conv2D(128, (3,3), padding='same', ))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    \n",
        "    model.add(Conv2D(128, (3,3), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Conv2D(256, (3,3), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_1 (Conv2D)           (None, 64, 64, 64)        1792      \n",
            "                                                                 \n",
            " leaky_re_lu_4 (LeakyReLU)   (None, 64, 64, 64)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 64, 64, 128)       73856     \n",
            "                                                                 \n",
            " leaky_re_lu_5 (LeakyReLU)   (None, 64, 64, 128)       0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 64, 64, 128)       147584    \n",
            "                                                                 \n",
            " leaky_re_lu_6 (LeakyReLU)   (None, 64, 64, 128)       0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 64, 64, 256)       295168    \n",
            "                                                                 \n",
            " leaky_re_lu_7 (LeakyReLU)   (None, 64, 64, 256)       0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1048576)           0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1048576)           0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 1048577   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,566,977\n",
            "Trainable params: 1,566,977\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbcKcKmA0q2S"
      },
      "source": [
        "## **5) Connecting Neural Networks to build GAN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0Ue3TEd0xLy"
      },
      "source": [
        "GAN = Sequential()\n",
        "discriminator.trainable = False\n",
        "GAN.add(generator)\n",
        "GAN.add(discriminator)\n",
        "\n",
        "GAN.compile(loss='binary_crossentropy', optimizer=adam)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPqU8dZDaQmE"
      },
      "source": [
        "# generator.summary()\n",
        "# discriminator.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WaNhBDwRwTG"
      },
      "source": [
        "## **6) Outputting Images**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQEJ0WbjRppy"
      },
      "source": [
        "#@title\n",
        "## **7) Outputting Images**\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import imageio\n",
        "import PIL\n",
        "\n",
        "save_name = 0.00000000\n",
        "\n",
        "def save_imgs(epoch):\n",
        "    r, c = 4, 4\n",
        "    noise = np.random.normal(0, 1, (r * c, latent_dim))\n",
        "    gen_imgs = generator.predict(noise)\n",
        "    global save_name\n",
        "    save_name += 0.00000001\n",
        "\n",
        "    # Rescale images 0 - 1\n",
        "    gen_imgs = (gen_imgs + 1) / 2.0\n",
        "\n",
        "    fig, axs = plt.subplots(r, c)\n",
        "    cnt = 0\n",
        "    for i in range(r):\n",
        "        for j in range(c):\n",
        "            axs[i,j].imshow(gen_imgs[cnt])\n",
        "            axs[i,j].axis('off')\n",
        "            cnt += 1\n",
        "    fig.savefig(\"currentgeneration.png\")\n",
        "    fig.savefig(\"generated_images/%.8f.png\" % save_name)\n",
        "    plt.close()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE57Lk5V0xs2"
      },
      "source": [
        "## **7) Training GAN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "egSJJvik00Iq",
        "outputId": "69d3ccdb-97b6-4600-f30d-0d5f0ef77299"
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "def train(epochs, batch_size=32, save_interval=200):\n",
        "\n",
        "  array = []\n",
        "  #PUT PATH OF RESIZED IMAGES\n",
        "  path = \"/content/resized_images/\"\n",
        "\n",
        "  for dir in os.listdir(path):\n",
        "            # print(dir)\n",
        "    image = Image.open(path + dir)\n",
        "    data = np.asarray(image)\n",
        "    array.append(data)\n",
        "\n",
        "  X_train = np.array(array)\n",
        "  print(X_train.shape)\n",
        "\n",
        "  # print(X_train.shape)\n",
        "  #Rescale data between -1 and 1\n",
        "  X_train = X_train / 127.5 -1.\n",
        "  bat_per_epo = int(X_train.shape[0] / batch_size)\n",
        "  # X_train = np.expand_dims(X_train, axis=3)\n",
        "  print(X_train.shape)\n",
        "\n",
        "  #Create our Y for our Neural Networks\n",
        "  valid = np.ones((batch_size, 1))\n",
        "  fakes = np.zeros((batch_size, 1))\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    for j in range(bat_per_epo):\n",
        "      #Get Random Batch\n",
        "      idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "      imgs = X_train[idx]\n",
        "\n",
        "      #Generate Fake Images\n",
        "      noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "      gen_imgs = generator.predict(noise)\n",
        "\n",
        "      #Train discriminator\n",
        "      d_loss_real = discriminator.train_on_batch(imgs, valid)\n",
        "      d_loss_fake = discriminator.train_on_batch(gen_imgs, fakes)\n",
        "      d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "      noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "      \n",
        "      #inverse y label\n",
        "      g_loss = GAN.train_on_batch(noise, valid)\n",
        "\n",
        "      print(\"******* %d %d [D loss: %f, acc: %.2f%%] [G loss: %f]\" % (epoch,j, d_loss[0], 100* d_loss[1], g_loss))\n",
        "\n",
        "      # if(epoch % save_interval) == 0:\n",
        "    save_imgs(epoch)\n",
        "\n",
        "\n",
        "train(30000, batch_size=64, save_interval=200)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "******* 230 57 [D loss: 0.007651, acc: 100.00%] [G loss: 6.923769]\n",
            "******* 230 58 [D loss: 0.017156, acc: 99.22%] [G loss: 7.407611]\n",
            "******* 230 59 [D loss: 0.044925, acc: 98.44%] [G loss: 7.034067]\n",
            "******* 230 60 [D loss: 0.030021, acc: 99.22%] [G loss: 6.921338]\n",
            "******* 230 61 [D loss: 0.029828, acc: 99.22%] [G loss: 5.959670]\n",
            "******* 230 62 [D loss: 0.043935, acc: 98.44%] [G loss: 6.882044]\n",
            "******* 230 63 [D loss: 0.011887, acc: 100.00%] [G loss: 6.658114]\n",
            "******* 230 64 [D loss: 0.013414, acc: 99.22%] [G loss: 7.888267]\n",
            "******* 230 65 [D loss: 0.082662, acc: 96.88%] [G loss: 7.090540]\n",
            "******* 230 66 [D loss: 0.022279, acc: 99.22%] [G loss: 7.111960]\n",
            "******* 230 67 [D loss: 0.023670, acc: 100.00%] [G loss: 7.429543]\n",
            "******* 230 68 [D loss: 0.024335, acc: 99.22%] [G loss: 6.663246]\n",
            "******* 230 69 [D loss: 0.067282, acc: 97.66%] [G loss: 6.584260]\n",
            "******* 230 70 [D loss: 0.031461, acc: 99.22%] [G loss: 8.051188]\n",
            "******* 230 71 [D loss: 0.015886, acc: 99.22%] [G loss: 9.049304]\n",
            "******* 230 72 [D loss: 0.005624, acc: 100.00%] [G loss: 10.647100]\n",
            "******* 230 73 [D loss: 0.007353, acc: 100.00%] [G loss: 11.070932]\n",
            "******* 230 74 [D loss: 0.085793, acc: 96.88%] [G loss: 9.982875]\n",
            "******* 231 0 [D loss: 0.013267, acc: 100.00%] [G loss: 7.470652]\n",
            "******* 231 1 [D loss: 0.049617, acc: 98.44%] [G loss: 7.273190]\n",
            "******* 231 2 [D loss: 0.019388, acc: 99.22%] [G loss: 7.806396]\n",
            "******* 231 3 [D loss: 0.002729, acc: 100.00%] [G loss: 9.039266]\n",
            "******* 231 4 [D loss: 0.014971, acc: 99.22%] [G loss: 9.855431]\n",
            "******* 231 5 [D loss: 0.027994, acc: 98.44%] [G loss: 9.187449]\n",
            "******* 231 6 [D loss: 0.024329, acc: 98.44%] [G loss: 8.512278]\n",
            "******* 231 7 [D loss: 0.005738, acc: 100.00%] [G loss: 8.127054]\n",
            "******* 231 8 [D loss: 0.011215, acc: 99.22%] [G loss: 7.612074]\n",
            "******* 231 9 [D loss: 0.008866, acc: 100.00%] [G loss: 7.316094]\n",
            "******* 231 10 [D loss: 0.003591, acc: 100.00%] [G loss: 7.329813]\n",
            "******* 231 11 [D loss: 0.010058, acc: 100.00%] [G loss: 7.435215]\n",
            "******* 231 12 [D loss: 0.008229, acc: 100.00%] [G loss: 8.194962]\n",
            "******* 231 13 [D loss: 0.016000, acc: 100.00%] [G loss: 8.506112]\n",
            "******* 231 14 [D loss: 0.004276, acc: 100.00%] [G loss: 9.049824]\n",
            "******* 231 15 [D loss: 0.029205, acc: 98.44%] [G loss: 8.289566]\n",
            "******* 231 16 [D loss: 0.008358, acc: 100.00%] [G loss: 8.360958]\n",
            "******* 231 17 [D loss: 0.022798, acc: 98.44%] [G loss: 7.788817]\n",
            "******* 231 18 [D loss: 0.037424, acc: 98.44%] [G loss: 8.296375]\n",
            "******* 231 19 [D loss: 0.022493, acc: 99.22%] [G loss: 8.931134]\n",
            "******* 231 20 [D loss: 0.014676, acc: 100.00%] [G loss: 9.100027]\n",
            "******* 231 21 [D loss: 0.119413, acc: 94.53%] [G loss: 7.814515]\n",
            "******* 231 22 [D loss: 0.035195, acc: 98.44%] [G loss: 7.158092]\n",
            "******* 231 23 [D loss: 0.045805, acc: 98.44%] [G loss: 5.731699]\n",
            "******* 231 24 [D loss: 0.017403, acc: 100.00%] [G loss: 5.865996]\n",
            "******* 231 25 [D loss: 0.101413, acc: 95.31%] [G loss: 6.689326]\n",
            "******* 231 26 [D loss: 0.002046, acc: 100.00%] [G loss: 9.896739]\n",
            "******* 231 27 [D loss: 0.109088, acc: 95.31%] [G loss: 10.639814]\n",
            "******* 231 28 [D loss: 0.184521, acc: 95.31%] [G loss: 5.994528]\n",
            "******* 231 29 [D loss: 0.079352, acc: 96.88%] [G loss: 5.446445]\n",
            "******* 231 30 [D loss: 0.077650, acc: 97.66%] [G loss: 6.125739]\n",
            "******* 231 31 [D loss: 0.015912, acc: 99.22%] [G loss: 8.158054]\n",
            "******* 231 32 [D loss: 0.038693, acc: 98.44%] [G loss: 9.533784]\n",
            "******* 231 33 [D loss: 0.020797, acc: 98.44%] [G loss: 10.330860]\n",
            "******* 231 34 [D loss: 0.032190, acc: 98.44%] [G loss: 9.338588]\n",
            "******* 231 35 [D loss: 0.032146, acc: 97.66%] [G loss: 9.012819]\n",
            "******* 231 36 [D loss: 0.004237, acc: 100.00%] [G loss: 9.054356]\n",
            "******* 231 37 [D loss: 0.003594, acc: 100.00%] [G loss: 7.805768]\n",
            "******* 231 38 [D loss: 0.007293, acc: 100.00%] [G loss: 7.367411]\n",
            "******* 231 39 [D loss: 0.009673, acc: 100.00%] [G loss: 6.933364]\n",
            "******* 231 40 [D loss: 0.007680, acc: 100.00%] [G loss: 7.515643]\n",
            "******* 231 41 [D loss: 0.071775, acc: 97.66%] [G loss: 7.943379]\n",
            "******* 231 42 [D loss: 0.009750, acc: 99.22%] [G loss: 9.647344]\n",
            "******* 231 43 [D loss: 0.077324, acc: 98.44%] [G loss: 9.419079]\n",
            "******* 231 44 [D loss: 0.045610, acc: 97.66%] [G loss: 8.100010]\n",
            "******* 231 45 [D loss: 0.009774, acc: 100.00%] [G loss: 7.311332]\n",
            "******* 231 46 [D loss: 0.049421, acc: 97.66%] [G loss: 5.648962]\n",
            "******* 231 47 [D loss: 0.083409, acc: 97.66%] [G loss: 5.773434]\n",
            "******* 231 48 [D loss: 0.026774, acc: 99.22%] [G loss: 6.984177]\n",
            "******* 231 49 [D loss: 0.004964, acc: 100.00%] [G loss: 7.362116]\n",
            "******* 231 50 [D loss: 0.051432, acc: 98.44%] [G loss: 8.483361]\n",
            "******* 231 51 [D loss: 0.053503, acc: 97.66%] [G loss: 7.095766]\n",
            "******* 231 52 [D loss: 0.029716, acc: 99.22%] [G loss: 6.667389]\n",
            "******* 231 53 [D loss: 0.018781, acc: 100.00%] [G loss: 7.405402]\n",
            "******* 231 54 [D loss: 0.025098, acc: 98.44%] [G loss: 7.338870]\n",
            "******* 231 55 [D loss: 0.018377, acc: 99.22%] [G loss: 7.860609]\n",
            "******* 231 56 [D loss: 0.021590, acc: 99.22%] [G loss: 7.410763]\n",
            "******* 231 57 [D loss: 0.023426, acc: 99.22%] [G loss: 6.991356]\n",
            "******* 231 58 [D loss: 0.006483, acc: 100.00%] [G loss: 6.709498]\n",
            "******* 231 59 [D loss: 0.051529, acc: 96.88%] [G loss: 8.499064]\n",
            "******* 231 60 [D loss: 0.002301, acc: 100.00%] [G loss: 9.594088]\n",
            "******* 231 61 [D loss: 0.015847, acc: 99.22%] [G loss: 9.720406]\n",
            "******* 231 62 [D loss: 0.022839, acc: 99.22%] [G loss: 9.660975]\n",
            "******* 231 63 [D loss: 0.034859, acc: 98.44%] [G loss: 7.984646]\n",
            "******* 231 64 [D loss: 0.065055, acc: 97.66%] [G loss: 7.455368]\n",
            "******* 231 65 [D loss: 0.017930, acc: 99.22%] [G loss: 7.175192]\n",
            "******* 231 66 [D loss: 0.015086, acc: 99.22%] [G loss: 6.914891]\n",
            "******* 231 67 [D loss: 0.010944, acc: 100.00%] [G loss: 6.828573]\n",
            "******* 231 68 [D loss: 0.015628, acc: 100.00%] [G loss: 7.004288]\n",
            "******* 231 69 [D loss: 0.039611, acc: 97.66%] [G loss: 7.075181]\n",
            "******* 231 70 [D loss: 0.010845, acc: 100.00%] [G loss: 8.342238]\n",
            "******* 231 71 [D loss: 0.003567, acc: 100.00%] [G loss: 8.902496]\n",
            "******* 231 72 [D loss: 0.008082, acc: 99.22%] [G loss: 10.149343]\n",
            "******* 231 73 [D loss: 0.036439, acc: 98.44%] [G loss: 10.690645]\n",
            "******* 231 74 [D loss: 0.000844, acc: 100.00%] [G loss: 10.308242]\n",
            "******* 232 0 [D loss: 0.021258, acc: 99.22%] [G loss: 10.053528]\n",
            "******* 232 1 [D loss: 0.009818, acc: 100.00%] [G loss: 9.257154]\n",
            "******* 232 2 [D loss: 0.005293, acc: 100.00%] [G loss: 8.845510]\n",
            "******* 232 3 [D loss: 0.011596, acc: 99.22%] [G loss: 8.170593]\n",
            "******* 232 4 [D loss: 0.010221, acc: 100.00%] [G loss: 8.648491]\n",
            "******* 232 5 [D loss: 0.011225, acc: 100.00%] [G loss: 8.910389]\n",
            "******* 232 6 [D loss: 0.019175, acc: 99.22%] [G loss: 9.112723]\n",
            "******* 232 7 [D loss: 0.004730, acc: 100.00%] [G loss: 8.500584]\n",
            "******* 232 8 [D loss: 0.013735, acc: 99.22%] [G loss: 8.045742]\n",
            "******* 232 9 [D loss: 0.055408, acc: 96.88%] [G loss: 7.320615]\n",
            "******* 232 10 [D loss: 0.051025, acc: 98.44%] [G loss: 7.338580]\n",
            "******* 232 11 [D loss: 0.037296, acc: 99.22%] [G loss: 8.130808]\n",
            "******* 232 12 [D loss: 0.033791, acc: 98.44%] [G loss: 9.044268]\n",
            "******* 232 13 [D loss: 0.062415, acc: 98.44%] [G loss: 9.564074]\n",
            "******* 232 14 [D loss: 0.036580, acc: 98.44%] [G loss: 11.572229]\n",
            "******* 232 15 [D loss: 0.070131, acc: 96.88%] [G loss: 10.770994]\n",
            "******* 232 16 [D loss: 0.058254, acc: 98.44%] [G loss: 7.978055]\n",
            "******* 232 17 [D loss: 0.088561, acc: 96.09%] [G loss: 7.393573]\n",
            "******* 232 18 [D loss: 0.122265, acc: 98.44%] [G loss: 7.562783]\n",
            "******* 232 19 [D loss: 0.066881, acc: 96.09%] [G loss: 9.225733]\n",
            "******* 232 20 [D loss: 0.033899, acc: 99.22%] [G loss: 10.051283]\n",
            "******* 232 21 [D loss: 0.010607, acc: 100.00%] [G loss: 9.503094]\n",
            "******* 232 22 [D loss: 0.059456, acc: 96.88%] [G loss: 9.368040]\n",
            "******* 232 23 [D loss: 0.040725, acc: 98.44%] [G loss: 8.655203]\n",
            "******* 232 24 [D loss: 0.004739, acc: 100.00%] [G loss: 9.054258]\n",
            "******* 232 25 [D loss: 0.011371, acc: 100.00%] [G loss: 8.558210]\n",
            "******* 232 26 [D loss: 0.014095, acc: 99.22%] [G loss: 8.740312]\n",
            "******* 232 27 [D loss: 0.088419, acc: 96.88%] [G loss: 7.199952]\n",
            "******* 232 28 [D loss: 0.015577, acc: 100.00%] [G loss: 5.806680]\n",
            "******* 232 29 [D loss: 0.049835, acc: 97.66%] [G loss: 7.346099]\n",
            "******* 232 30 [D loss: 0.041056, acc: 98.44%] [G loss: 8.100948]\n",
            "******* 232 31 [D loss: 0.051386, acc: 96.88%] [G loss: 7.859315]\n",
            "******* 232 32 [D loss: 0.025024, acc: 99.22%] [G loss: 8.649721]\n",
            "******* 232 33 [D loss: 0.032121, acc: 99.22%] [G loss: 7.641653]\n",
            "******* 232 34 [D loss: 0.080386, acc: 97.66%] [G loss: 6.268902]\n",
            "******* 232 35 [D loss: 0.130850, acc: 94.53%] [G loss: 7.059978]\n",
            "******* 232 36 [D loss: 0.057085, acc: 98.44%] [G loss: 9.590022]\n",
            "******* 232 37 [D loss: 0.037480, acc: 98.44%] [G loss: 10.535443]\n",
            "******* 232 38 [D loss: 0.094012, acc: 96.09%] [G loss: 7.648850]\n",
            "******* 232 39 [D loss: 0.016203, acc: 100.00%] [G loss: 5.850102]\n",
            "******* 232 40 [D loss: 0.084312, acc: 97.66%] [G loss: 6.177114]\n",
            "******* 232 41 [D loss: 0.034171, acc: 98.44%] [G loss: 9.365532]\n",
            "******* 232 42 [D loss: 0.001797, acc: 100.00%] [G loss: 10.799446]\n",
            "******* 232 43 [D loss: 0.021774, acc: 99.22%] [G loss: 11.908670]\n",
            "******* 232 44 [D loss: 0.129857, acc: 96.09%] [G loss: 10.758339]\n",
            "******* 232 45 [D loss: 0.134336, acc: 94.53%] [G loss: 8.641555]\n",
            "******* 232 46 [D loss: 0.039408, acc: 98.44%] [G loss: 7.402908]\n",
            "******* 232 47 [D loss: 0.034192, acc: 99.22%] [G loss: 7.001073]\n",
            "******* 232 48 [D loss: 0.089234, acc: 97.66%] [G loss: 7.877030]\n",
            "******* 232 49 [D loss: 0.008049, acc: 100.00%] [G loss: 10.271437]\n",
            "******* 232 50 [D loss: 0.002893, acc: 100.00%] [G loss: 12.371761]\n",
            "******* 232 51 [D loss: 0.025965, acc: 98.44%] [G loss: 11.869278]\n",
            "******* 232 52 [D loss: 0.004939, acc: 100.00%] [G loss: 11.359669]\n",
            "******* 232 53 [D loss: 0.051977, acc: 99.22%] [G loss: 10.776646]\n",
            "******* 232 54 [D loss: 0.003901, acc: 100.00%] [G loss: 8.982594]\n",
            "******* 232 55 [D loss: 0.007148, acc: 100.00%] [G loss: 8.485304]\n",
            "******* 232 56 [D loss: 0.094022, acc: 97.66%] [G loss: 7.964030]\n",
            "******* 232 57 [D loss: 0.016400, acc: 99.22%] [G loss: 8.346752]\n",
            "******* 232 58 [D loss: 0.001474, acc: 100.00%] [G loss: 7.901322]\n",
            "******* 232 59 [D loss: 0.008762, acc: 100.00%] [G loss: 7.949638]\n",
            "******* 232 60 [D loss: 0.013140, acc: 100.00%] [G loss: 7.556999]\n",
            "******* 232 61 [D loss: 0.017941, acc: 100.00%] [G loss: 7.580760]\n",
            "******* 232 62 [D loss: 0.012464, acc: 99.22%] [G loss: 6.715549]\n",
            "******* 232 63 [D loss: 0.031228, acc: 98.44%] [G loss: 6.803593]\n",
            "******* 232 64 [D loss: 0.020234, acc: 99.22%] [G loss: 7.084360]\n",
            "******* 232 65 [D loss: 0.042350, acc: 98.44%] [G loss: 6.994770]\n",
            "******* 232 66 [D loss: 0.043809, acc: 98.44%] [G loss: 7.043024]\n",
            "******* 232 67 [D loss: 0.063663, acc: 96.88%] [G loss: 7.182267]\n",
            "******* 232 68 [D loss: 0.079654, acc: 96.88%] [G loss: 7.425135]\n",
            "******* 232 69 [D loss: 0.010131, acc: 99.22%] [G loss: 9.778289]\n",
            "******* 232 70 [D loss: 0.035865, acc: 98.44%] [G loss: 9.014738]\n",
            "******* 232 71 [D loss: 0.017359, acc: 99.22%] [G loss: 9.209160]\n",
            "******* 232 72 [D loss: 0.019133, acc: 99.22%] [G loss: 8.567512]\n",
            "******* 232 73 [D loss: 0.007044, acc: 100.00%] [G loss: 7.921600]\n",
            "******* 232 74 [D loss: 0.049401, acc: 99.22%] [G loss: 7.083278]\n",
            "******* 233 0 [D loss: 0.036507, acc: 98.44%] [G loss: 7.248576]\n",
            "******* 233 1 [D loss: 0.027739, acc: 99.22%] [G loss: 7.909303]\n",
            "******* 233 2 [D loss: 0.003131, acc: 100.00%] [G loss: 9.124906]\n",
            "******* 233 3 [D loss: 0.019416, acc: 98.44%] [G loss: 8.546638]\n",
            "******* 233 4 [D loss: 0.025113, acc: 99.22%] [G loss: 8.900156]\n",
            "******* 233 5 [D loss: 0.005685, acc: 100.00%] [G loss: 8.793907]\n",
            "******* 233 6 [D loss: 0.017039, acc: 99.22%] [G loss: 8.321074]\n",
            "******* 233 7 [D loss: 0.095258, acc: 98.44%] [G loss: 7.489660]\n",
            "******* 233 8 [D loss: 0.026399, acc: 99.22%] [G loss: 7.130447]\n",
            "******* 233 9 [D loss: 0.038535, acc: 99.22%] [G loss: 6.436074]\n",
            "******* 233 10 [D loss: 0.024422, acc: 100.00%] [G loss: 7.035512]\n",
            "******* 233 11 [D loss: 0.008004, acc: 100.00%] [G loss: 7.085761]\n",
            "******* 233 12 [D loss: 0.007379, acc: 100.00%] [G loss: 7.557900]\n",
            "******* 233 13 [D loss: 0.067954, acc: 98.44%] [G loss: 8.005412]\n",
            "******* 233 14 [D loss: 0.065088, acc: 98.44%] [G loss: 8.411301]\n",
            "******* 233 15 [D loss: 0.017303, acc: 98.44%] [G loss: 7.380792]\n",
            "******* 233 16 [D loss: 0.038758, acc: 99.22%] [G loss: 7.746540]\n",
            "******* 233 17 [D loss: 0.065393, acc: 97.66%] [G loss: 7.981147]\n",
            "******* 233 18 [D loss: 0.029223, acc: 99.22%] [G loss: 8.399808]\n",
            "******* 233 19 [D loss: 0.050490, acc: 98.44%] [G loss: 8.294285]\n",
            "******* 233 20 [D loss: 0.017485, acc: 99.22%] [G loss: 8.065861]\n",
            "******* 233 21 [D loss: 0.081433, acc: 96.88%] [G loss: 6.972270]\n",
            "******* 233 22 [D loss: 0.028484, acc: 99.22%] [G loss: 6.442640]\n",
            "******* 233 23 [D loss: 0.012896, acc: 100.00%] [G loss: 6.986371]\n",
            "******* 233 24 [D loss: 0.004117, acc: 100.00%] [G loss: 6.933010]\n",
            "******* 233 25 [D loss: 0.072315, acc: 96.88%] [G loss: 6.711032]\n",
            "******* 233 26 [D loss: 0.013733, acc: 100.00%] [G loss: 7.126411]\n",
            "******* 233 27 [D loss: 0.002455, acc: 100.00%] [G loss: 8.013705]\n",
            "******* 233 28 [D loss: 0.002406, acc: 100.00%] [G loss: 8.888734]\n",
            "******* 233 29 [D loss: 0.016034, acc: 99.22%] [G loss: 9.077780]\n",
            "******* 233 30 [D loss: 0.036511, acc: 98.44%] [G loss: 7.916497]\n",
            "******* 233 31 [D loss: 0.017612, acc: 100.00%] [G loss: 7.912291]\n",
            "******* 233 32 [D loss: 0.009176, acc: 100.00%] [G loss: 8.022865]\n",
            "******* 233 33 [D loss: 0.017530, acc: 99.22%] [G loss: 8.189313]\n",
            "******* 233 34 [D loss: 0.004159, acc: 100.00%] [G loss: 9.014280]\n",
            "******* 233 35 [D loss: 0.016896, acc: 99.22%] [G loss: 8.832920]\n",
            "******* 233 36 [D loss: 0.002325, acc: 100.00%] [G loss: 8.730931]\n",
            "******* 233 37 [D loss: 0.007744, acc: 100.00%] [G loss: 9.544113]\n",
            "******* 233 38 [D loss: 0.030475, acc: 98.44%] [G loss: 9.638692]\n",
            "******* 233 39 [D loss: 0.078833, acc: 97.66%] [G loss: 8.330935]\n",
            "******* 233 40 [D loss: 0.011267, acc: 99.22%] [G loss: 7.483372]\n",
            "******* 233 41 [D loss: 0.029997, acc: 97.66%] [G loss: 8.036894]\n",
            "******* 233 42 [D loss: 0.032120, acc: 98.44%] [G loss: 8.439381]\n",
            "******* 233 43 [D loss: 0.043368, acc: 99.22%] [G loss: 8.763695]\n",
            "******* 233 44 [D loss: 0.040359, acc: 98.44%] [G loss: 9.001191]\n",
            "******* 233 45 [D loss: 0.040339, acc: 98.44%] [G loss: 8.329484]\n",
            "******* 233 46 [D loss: 0.086350, acc: 98.44%] [G loss: 7.811009]\n",
            "******* 233 47 [D loss: 0.002327, acc: 100.00%] [G loss: 7.982115]\n",
            "******* 233 48 [D loss: 0.024357, acc: 98.44%] [G loss: 8.223423]\n",
            "******* 233 49 [D loss: 0.057789, acc: 99.22%] [G loss: 7.533135]\n",
            "******* 233 50 [D loss: 0.002785, acc: 100.00%] [G loss: 6.901469]\n",
            "******* 233 51 [D loss: 0.009275, acc: 100.00%] [G loss: 7.143913]\n",
            "******* 233 52 [D loss: 0.019957, acc: 99.22%] [G loss: 7.087844]\n",
            "******* 233 53 [D loss: 0.114554, acc: 95.31%] [G loss: 5.811140]\n",
            "******* 233 54 [D loss: 0.116173, acc: 96.09%] [G loss: 6.170844]\n",
            "******* 233 55 [D loss: 0.002172, acc: 100.00%] [G loss: 8.330664]\n",
            "******* 233 56 [D loss: 0.005024, acc: 100.00%] [G loss: 10.014528]\n",
            "******* 233 57 [D loss: 0.044500, acc: 97.66%] [G loss: 9.126165]\n",
            "******* 233 58 [D loss: 0.118870, acc: 96.09%] [G loss: 8.145255]\n",
            "******* 233 59 [D loss: 0.003572, acc: 100.00%] [G loss: 7.474731]\n",
            "******* 233 60 [D loss: 0.012736, acc: 99.22%] [G loss: 7.021174]\n",
            "******* 233 61 [D loss: 0.027346, acc: 99.22%] [G loss: 7.565485]\n",
            "******* 233 62 [D loss: 0.020045, acc: 99.22%] [G loss: 7.608655]\n",
            "******* 233 63 [D loss: 0.112523, acc: 96.88%] [G loss: 7.900331]\n",
            "******* 233 64 [D loss: 0.018634, acc: 99.22%] [G loss: 10.041183]\n",
            "******* 233 65 [D loss: 0.060020, acc: 97.66%] [G loss: 10.303255]\n",
            "******* 233 66 [D loss: 0.062563, acc: 96.88%] [G loss: 9.009924]\n",
            "******* 233 67 [D loss: 0.022906, acc: 99.22%] [G loss: 9.209077]\n",
            "******* 233 68 [D loss: 0.036402, acc: 98.44%] [G loss: 8.329177]\n",
            "******* 233 69 [D loss: 0.040303, acc: 99.22%] [G loss: 7.877750]\n",
            "******* 233 70 [D loss: 0.070435, acc: 98.44%] [G loss: 8.585472]\n",
            "******* 233 71 [D loss: 0.049459, acc: 98.44%] [G loss: 9.969563]\n",
            "******* 233 72 [D loss: 0.014527, acc: 100.00%] [G loss: 9.374784]\n",
            "******* 233 73 [D loss: 0.018041, acc: 99.22%] [G loss: 8.961237]\n",
            "******* 233 74 [D loss: 0.089114, acc: 97.66%] [G loss: 6.673180]\n",
            "******* 234 0 [D loss: 0.034955, acc: 99.22%] [G loss: 5.759294]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-1429e7613df2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-1429e7613df2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, batch_size, save_interval)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0;31m#Train discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m       \u001b[0md_loss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m       \u001b[0md_loss_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfakes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m       \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_loss_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   1900\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1902\u001b[0;31m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1903\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1904\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    548\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1147\u001b[0m     \"\"\"\n\u001b[1;32m   1148\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1113\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1115\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1116\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kT7_Wk-TS_n2"
      },
      "source": [
        "noise = np.random.normal(0, 1, (234, latent_dim))\n",
        "gen_imgs = generator.predict(noise)\n",
        "gen_imgs = (gen_imgs + 1) / 2.0\n",
        "# plt.imshow(gen_imgs[2])"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbWAZ1v_TdJd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "outputId": "e8ed9172-d460-487d-cdb2-b78b9dd67bfb"
      },
      "source": [
        "plt.imshow(gen_imgs[163])"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f712da3ca10>"
            ]
          },
          "metadata": {},
          "execution_count": 123
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8YtpQpkRvRI"
      },
      "source": [
        "generator.save_weights(\"/content/drive/MyDrive/generator1hour.h5\")\n",
        "discriminator.save_weights(\"/content/drive/MyDrive/discriminator1hour.h5\")"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "po-jSQoN1Azl"
      },
      "source": [
        "### **8) Making GIF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPShgQpg1EMy"
      },
      "source": [
        " #Display a single image using the epoch number\n",
        " def display_image(epoch_no):\n",
        "   return PIL.Image.open('generated_images/%.8f.png'.format(epoch_no))\n",
        "\n",
        "anim_file = 'paintgan.gif'\n",
        "\n",
        "with imageio.get_writer(anim_file, mode='I') as writer:\n",
        "  filenames = glob.glob('generated_images/*.png')\n",
        "  filenames = sorted(filenames)\n",
        "  for filename in filenames:\n",
        "    image = imageio.imread(filename)\n",
        "    writer.append_data(image)\n",
        "  image = imageio.imread(filename)\n",
        "  writer.append_data(image)"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogrmQ73ZR_Wi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}